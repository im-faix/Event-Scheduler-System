
==> Audit <==
|----------------|-------------------------|----------|--------|---------|---------------------|---------------------|
|    Command     |          Args           | Profile  |  User  | Version |     Start Time      |      End Time       |
|----------------|-------------------------|----------|--------|---------|---------------------|---------------------|
| update-context |                         | minikube | faizan | v1.36.0 | 25 Jul 25 10:14 UTC | 25 Jul 25 10:14 UTC |
| image          | load event-scheduler    | minikube | faizan | v1.36.0 | 25 Jul 25 10:15 UTC |                     |
| image          | load event_new:v1       | minikube | faizan | v1.36.0 | 25 Jul 25 10:15 UTC | 25 Jul 25 10:15 UTC |
| ip             |                         | minikube | faizan | v1.36.0 | 25 Jul 25 10:20 UTC | 25 Jul 25 10:20 UTC |
| start          |                         | minikube | faizan | v1.36.0 | 25 Jul 25 12:52 UTC | 25 Jul 25 12:54 UTC |
| start          |                         | minikube | faizan | v1.36.0 | 28 Jul 25 04:33 UTC | 28 Jul 25 04:33 UTC |
| ip             |                         | minikube | faizan | v1.36.0 | 28 Jul 25 04:52 UTC | 28 Jul 25 04:52 UTC |
| stop           |                         | minikube | faizan | v1.36.0 | 28 Jul 25 09:10 UTC | 28 Jul 25 09:10 UTC |
| start          |                         | minikube | faizan | v1.36.0 | 28 Jul 25 10:19 UTC | 28 Jul 25 10:19 UTC |
| image          | load productapp:latest  | minikube | faizan | v1.36.0 | 28 Jul 25 10:26 UTC | 28 Jul 25 10:27 UTC |
| docker-env     |                         | minikube | faizan | v1.36.0 | 28 Jul 25 10:30 UTC | 28 Jul 25 10:30 UTC |
| docker-env     |                         | minikube | faizan | v1.36.0 | 28 Jul 25 10:30 UTC | 28 Jul 25 10:30 UTC |
| image          | load productapp:latest  | minikube | faizan | v1.36.0 | 28 Jul 25 10:31 UTC |                     |
| image          | load product-app:latest | minikube | faizan | v1.36.0 | 28 Jul 25 10:31 UTC | 28 Jul 25 10:32 UTC |
| stop           |                         | minikube | faizan | v1.36.0 | 28 Jul 25 10:33 UTC | 28 Jul 25 10:33 UTC |
| docker-env     |                         | minikube | faizan | v1.36.0 | 28 Jul 25 10:33 UTC |                     |
| start          |                         | minikube | faizan | v1.36.0 | 28 Jul 25 10:34 UTC | 28 Jul 25 10:34 UTC |
| docker-env     |                         | minikube | faizan | v1.36.0 | 28 Jul 25 10:34 UTC | 28 Jul 25 10:34 UTC |
| stop           |                         | minikube | faizan | v1.36.0 | 28 Jul 25 10:38 UTC | 28 Jul 25 10:39 UTC |
| docker-env     | -u                      | minikube | faizan | v1.36.0 | 28 Jul 25 10:39 UTC | 28 Jul 25 10:39 UTC |
| start          |                         | minikube | faizan | v1.36.0 | 29 Jul 25 04:25 UTC | 29 Jul 25 04:25 UTC |
| dashboard      |                         | minikube | faizan | v1.36.0 | 29 Jul 25 04:30 UTC |                     |
| dashboard      |                         | minikube | faizan | v1.36.0 | 29 Jul 25 04:33 UTC |                     |
| ssh-key        |                         | minikube | faizan | v1.36.0 | 29 Jul 25 04:40 UTC | 29 Jul 25 04:40 UTC |
| options        |                         | minikube | faizan | v1.36.0 | 29 Jul 25 04:41 UTC | 29 Jul 25 04:41 UTC |
| addons         | enable ingress          | minikube | faizan | v1.36.0 | 29 Jul 25 05:07 UTC |                     |
| addons         |                         | minikube | faizan | v1.36.0 | 29 Jul 25 05:10 UTC | 29 Jul 25 05:10 UTC |
| addons         |                         | minikube | faizan | v1.36.0 | 29 Jul 25 05:13 UTC | 29 Jul 25 05:13 UTC |
| addons         |                         | minikube | faizan | v1.36.0 | 29 Jul 25 05:13 UTC | 29 Jul 25 05:13 UTC |
| addons         | enable ingress          | minikube | faizan | v1.36.0 | 29 Jul 25 05:13 UTC |                     |
| addons         | ls                      | minikube | faizan | v1.36.0 | 29 Jul 25 05:30 UTC | 29 Jul 25 05:30 UTC |
| options        |                         | minikube | faizan | v1.36.0 | 29 Jul 25 05:31 UTC | 29 Jul 25 05:31 UTC |
| dashboard      |                         | minikube | faizan | v1.36.0 | 29 Jul 25 05:50 UTC |                     |
| start          |                         | minikube | faizan | v1.36.0 | 31 Jul 25 04:44 UTC | 31 Jul 25 04:44 UTC |
| start          |                         | minikube | faizan | v1.36.0 | 31 Jul 25 09:51 UTC | 31 Jul 25 09:51 UTC |
| start          |                         | minikube | faizan | v1.36.0 | 01 Aug 25 04:45 UTC | 01 Aug 25 04:46 UTC |
| start          |                         | minikube | faizan | v1.36.0 | 01 Aug 25 13:04 UTC | 01 Aug 25 13:05 UTC |
| image          | load productapp:latest  | minikube | faizan | v1.36.0 | 01 Aug 25 13:22 UTC | 01 Aug 25 13:23 UTC |
| start          |                         | minikube | faizan | v1.36.0 | 04 Aug 25 07:27 UTC | 04 Aug 25 07:27 UTC |
| ip             |                         | minikube | faizan | v1.36.0 | 04 Aug 25 07:36 UTC | 04 Aug 25 07:36 UTC |
| ip             |                         | minikube | faizan | v1.36.0 | 04 Aug 25 09:05 UTC | 04 Aug 25 09:05 UTC |
| dashboard      |                         | minikube | faizan | v1.36.0 | 04 Aug 25 10:53 UTC |                     |
| ssh            |                         | minikube | faizan | v1.36.0 | 04 Aug 25 12:18 UTC | 05 Aug 25 04:29 UTC |
| start          |                         | minikube | faizan | v1.36.0 | 05 Aug 25 09:17 UTC | 05 Aug 25 09:17 UTC |
| cache          | add product-app:latest  | minikube | faizan | v1.36.0 | 05 Aug 25 10:33 UTC |                     |
| image          | load product-app:latest | minikube | faizan | v1.36.0 | 05 Aug 25 10:33 UTC | 05 Aug 25 10:34 UTC |
| ssh            |                         | minikube | faizan | v1.36.0 | 05 Aug 25 10:37 UTC | 05 Aug 25 10:39 UTC |
| addons         | enable ingress          | minikube | faizan | v1.36.0 | 05 Aug 25 11:12 UTC | 05 Aug 25 11:12 UTC |
| ip             |                         | minikube | faizan | v1.36.0 | 05 Aug 25 11:14 UTC | 05 Aug 25 11:14 UTC |
| ip             |                         | minikube | faizan | v1.36.0 | 05 Aug 25 11:16 UTC | 05 Aug 25 11:16 UTC |
| tunnel         |                         | minikube | faizan | v1.36.0 | 05 Aug 25 11:43 UTC |                     |
| ip             |                         | minikube | faizan | v1.36.0 | 05 Aug 25 12:11 UTC | 05 Aug 25 12:11 UTC |
| stop           |                         | minikube | faizan | v1.36.0 | 05 Aug 25 12:22 UTC | 05 Aug 25 12:22 UTC |
| ssh            |                         | minikube | faizan | v1.36.0 | 05 Aug 25 12:57 UTC |                     |
| start          |                         | minikube | faizan | v1.36.0 | 05 Aug 25 12:57 UTC | 05 Aug 25 12:57 UTC |
| ssh            |                         | minikube | faizan | v1.36.0 | 05 Aug 25 12:58 UTC | 05 Aug 25 13:01 UTC |
| start          |                         | minikube | faizan | v1.36.0 | 06 Aug 25 05:13 UTC | 06 Aug 25 05:13 UTC |
| ip             |                         | minikube | faizan | v1.36.0 | 06 Aug 25 05:14 UTC | 06 Aug 25 05:14 UTC |
| service        | event --namespace=dev   | minikube | faizan | v1.36.0 | 06 Aug 25 05:15 UTC |                     |
| service        | event --namespace=dev   | minikube | faizan | v1.36.0 | 06 Aug 25 05:15 UTC |                     |
|----------------|-------------------------|----------|--------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/08/06 05:13:32
Running on machine: FFE-Mohammed-Faizan
Binary: Built with gc go1.24.0 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0806 05:13:32.142781    1346 out.go:345] Setting OutFile to fd 1 ...
I0806 05:13:32.144173    1346 out.go:397] isatty.IsTerminal(1) = true
I0806 05:13:32.144182    1346 out.go:358] Setting ErrFile to fd 2...
I0806 05:13:32.144189    1346 out.go:397] isatty.IsTerminal(2) = true
I0806 05:13:32.144735    1346 root.go:338] Updating PATH: /home/faizan/.minikube/bin
W0806 05:13:32.145962    1346 root.go:314] Error reading config file at /home/faizan/.minikube/config/config.json: open /home/faizan/.minikube/config/config.json: no such file or directory
I0806 05:13:32.147663    1346 out.go:352] Setting JSON to false
I0806 05:13:32.148601    1346 start.go:130] hostinfo: {"hostname":"FFE-Mohammed-Faizan","uptime":1736,"bootTime":1754455476,"procs":33,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.6.87.2-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"guest","hostId":"6bb9a96e-878d-4546-8924-89dc0372308e"}
I0806 05:13:32.150378    1346 start.go:140] virtualization: kvm guest
I0806 05:13:32.153284    1346 out.go:177] üòÑ  minikube v1.36.0 on Ubuntu 24.04 (kvm/amd64)
I0806 05:13:32.157721    1346 notify.go:220] Checking for updates...
I0806 05:13:32.158118    1346 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0806 05:13:32.159015    1346 driver.go:404] Setting default libvirt URI to qemu:///system
I0806 05:13:32.362387    1346 docker.go:123] docker version: linux-28.3.2:Docker Engine - Community
I0806 05:13:32.362484    1346 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0806 05:13:32.670768    1346 info.go:266] docker info: {ID:18fc3880-349d-464e-b5e8-ae4fb6b16014 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:43 SystemTime:2025-08-06 05:13:32.651934136 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8193798144 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:FFE-Mohammed-Faizan Labels:[] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2]] Warnings:<nil>}}
I0806 05:13:32.671008    1346 docker.go:318] overlay module found
I0806 05:13:32.674102    1346 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0806 05:13:32.677380    1346 start.go:304] selected driver: docker
I0806 05:13:32.677391    1346 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.47 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/faizan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0806 05:13:32.677538    1346 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0806 05:13:32.677915    1346 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0806 05:13:32.780549    1346 info.go:266] docker info: {ID:18fc3880-349d-464e-b5e8-ae4fb6b16014 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:43 SystemTime:2025-08-06 05:13:32.765092594 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8193798144 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:FFE-Mohammed-Faizan Labels:[] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2]] Warnings:<nil>}}
I0806 05:13:32.781919    1346 cni.go:84] Creating CNI manager for ""
I0806 05:13:32.782005    1346 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0806 05:13:32.782072    1346 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.47 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/faizan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0806 05:13:32.784348    1346 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0806 05:13:32.786722    1346 cache.go:121] Beginning downloading kic base image for docker with docker
I0806 05:13:32.790823    1346 out.go:177] üöú  Pulling base image v0.0.47 ...
I0806 05:13:32.794801    1346 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0806 05:13:32.795016    1346 image.go:81] Checking for docker.io/kicbase/stable:v0.0.47 in local docker daemon
I0806 05:13:32.796255    1346 preload.go:146] Found local preload: /home/faizan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0806 05:13:32.796270    1346 cache.go:56] Caching tarball of preloaded images
I0806 05:13:32.798216    1346 preload.go:172] Found /home/faizan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0806 05:13:32.798244    1346 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0806 05:13:32.798422    1346 profile.go:143] Saving config to /home/faizan/.minikube/profiles/minikube/config.json ...
I0806 05:13:32.846899    1346 image.go:100] Found docker.io/kicbase/stable:v0.0.47 in local docker daemon, skipping pull
I0806 05:13:32.846916    1346 cache.go:145] docker.io/kicbase/stable:v0.0.47 exists in daemon, skipping load
I0806 05:13:32.846965    1346 cache.go:230] Successfully downloaded all kic artifacts
I0806 05:13:32.846995    1346 start.go:360] acquireMachinesLock for minikube: {Name:mkcc279361b14a0ab48bb1f8c88c0aac6835f7eb Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0806 05:13:32.847186    1346 start.go:364] duration metric: took 171.034¬µs to acquireMachinesLock for "minikube"
I0806 05:13:32.847208    1346 start.go:96] Skipping create...Using existing machine configuration
I0806 05:13:32.847213    1346 fix.go:54] fixHost starting: 
I0806 05:13:32.847512    1346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0806 05:13:32.886997    1346 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0806 05:13:32.887027    1346 fix.go:138] unexpected machine state, will restart: <nil>
I0806 05:13:32.890759    1346 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0806 05:13:32.894226    1346 cli_runner.go:164] Run: docker start minikube
I0806 05:13:33.512184    1346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0806 05:13:33.535962    1346 kic.go:430] container "minikube" state is running.
I0806 05:13:33.536490    1346 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0806 05:13:33.559018    1346 profile.go:143] Saving config to /home/faizan/.minikube/profiles/minikube/config.json ...
I0806 05:13:33.559351    1346 machine.go:93] provisionDockerMachine start ...
I0806 05:13:33.559480    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:33.583538    1346 main.go:141] libmachine: Using SSH client type: native
I0806 05:13:33.584567    1346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0806 05:13:33.584588    1346 main.go:141] libmachine: About to run SSH command:
hostname
I0806 05:13:33.585828    1346 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:50246->127.0.0.1:32768: read: connection reset by peer
I0806 05:13:36.757391    1346 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0806 05:13:36.757491    1346 ubuntu.go:169] provisioning hostname "minikube"
I0806 05:13:36.757628    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:36.798285    1346 main.go:141] libmachine: Using SSH client type: native
I0806 05:13:36.799004    1346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0806 05:13:36.799022    1346 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0806 05:13:37.034596    1346 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0806 05:13:37.034796    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:37.075699    1346 main.go:141] libmachine: Using SSH client type: native
I0806 05:13:37.076118    1346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0806 05:13:37.076147    1346 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0806 05:13:37.250288    1346 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0806 05:13:37.250405    1346 ubuntu.go:175] set auth options {CertDir:/home/faizan/.minikube CaCertPath:/home/faizan/.minikube/certs/ca.pem CaPrivateKeyPath:/home/faizan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/faizan/.minikube/machines/server.pem ServerKeyPath:/home/faizan/.minikube/machines/server-key.pem ClientKeyPath:/home/faizan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/faizan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/faizan/.minikube}
I0806 05:13:37.250429    1346 ubuntu.go:177] setting up certificates
I0806 05:13:37.250482    1346 provision.go:84] configureAuth start
I0806 05:13:37.250683    1346 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0806 05:13:37.288389    1346 provision.go:143] copyHostCerts
I0806 05:13:37.298246    1346 exec_runner.go:144] found /home/faizan/.minikube/ca.pem, removing ...
I0806 05:13:37.299493    1346 exec_runner.go:203] rm: /home/faizan/.minikube/ca.pem
I0806 05:13:37.299729    1346 exec_runner.go:151] cp: /home/faizan/.minikube/certs/ca.pem --> /home/faizan/.minikube/ca.pem (1078 bytes)
I0806 05:13:37.300928    1346 exec_runner.go:144] found /home/faizan/.minikube/cert.pem, removing ...
I0806 05:13:37.300937    1346 exec_runner.go:203] rm: /home/faizan/.minikube/cert.pem
I0806 05:13:37.301142    1346 exec_runner.go:151] cp: /home/faizan/.minikube/certs/cert.pem --> /home/faizan/.minikube/cert.pem (1123 bytes)
I0806 05:13:37.302205    1346 exec_runner.go:144] found /home/faizan/.minikube/key.pem, removing ...
I0806 05:13:37.302218    1346 exec_runner.go:203] rm: /home/faizan/.minikube/key.pem
I0806 05:13:37.302295    1346 exec_runner.go:151] cp: /home/faizan/.minikube/certs/key.pem --> /home/faizan/.minikube/key.pem (1675 bytes)
I0806 05:13:37.302972    1346 provision.go:117] generating server cert: /home/faizan/.minikube/machines/server.pem ca-key=/home/faizan/.minikube/certs/ca.pem private-key=/home/faizan/.minikube/certs/ca-key.pem org=faizan.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0806 05:13:37.961272    1346 provision.go:177] copyRemoteCerts
I0806 05:13:37.961498    1346 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0806 05:13:37.961768    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:37.994352    1346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/faizan/.minikube/machines/minikube/id_rsa Username:docker}
I0806 05:13:38.113860    1346 ssh_runner.go:362] scp /home/faizan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0806 05:13:38.189582    1346 ssh_runner.go:362] scp /home/faizan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0806 05:13:38.245291    1346 ssh_runner.go:362] scp /home/faizan/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0806 05:13:38.289505    1346 provision.go:87] duration metric: took 1.038967188s to configureAuth
I0806 05:13:38.289541    1346 ubuntu.go:193] setting minikube options for container-runtime
I0806 05:13:38.289916    1346 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0806 05:13:38.289982    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:38.315567    1346 main.go:141] libmachine: Using SSH client type: native
I0806 05:13:38.315946    1346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0806 05:13:38.315956    1346 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0806 05:13:39.834868    1346 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0806 05:13:39.834893    1346 ubuntu.go:71] root file system type: overlay
I0806 05:13:39.835270    1346 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0806 05:13:39.835390    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:39.876032    1346 main.go:141] libmachine: Using SSH client type: native
I0806 05:13:39.876471    1346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0806 05:13:39.876568    1346 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0806 05:13:40.090247    1346 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0806 05:13:40.090463    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:40.127975    1346 main.go:141] libmachine: Using SSH client type: native
I0806 05:13:40.128448    1346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0806 05:13:40.128467    1346 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0806 05:13:40.316781    1346 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0806 05:13:40.316805    1346 machine.go:96] duration metric: took 5.41096211s to provisionDockerMachine
I0806 05:13:40.316837    1346 start.go:293] postStartSetup for "minikube" (driver="docker")
I0806 05:13:40.316856    1346 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0806 05:13:40.316986    1346 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0806 05:13:40.317076    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:40.353183    1346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/faizan/.minikube/machines/minikube/id_rsa Username:docker}
I0806 05:13:40.488089    1346 ssh_runner.go:195] Run: cat /etc/os-release
I0806 05:13:40.495664    1346 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0806 05:13:40.495718    1346 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0806 05:13:40.495739    1346 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0806 05:13:40.495751    1346 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0806 05:13:40.495769    1346 filesync.go:126] Scanning /home/faizan/.minikube/addons for local assets ...
I0806 05:13:40.496904    1346 filesync.go:126] Scanning /home/faizan/.minikube/files for local assets ...
I0806 05:13:40.497476    1346 start.go:296] duration metric: took 180.621218ms for postStartSetup
I0806 05:13:40.497687    1346 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0806 05:13:40.497770    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:40.531070    1346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/faizan/.minikube/machines/minikube/id_rsa Username:docker}
I0806 05:13:40.647203    1346 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0806 05:13:40.656478    1346 fix.go:56] duration metric: took 6.462774408s for fixHost
I0806 05:13:40.656498    1346 start.go:83] releasing machines lock for "minikube", held for 6.462822391s
I0806 05:13:40.656673    1346 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0806 05:13:40.681951    1346 ssh_runner.go:195] Run: cat /version.json
I0806 05:13:40.682040    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:40.682127    1346 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0806 05:13:40.682225    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:40.710399    1346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/faizan/.minikube/machines/minikube/id_rsa Username:docker}
I0806 05:13:40.712097    1346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/faizan/.minikube/machines/minikube/id_rsa Username:docker}
I0806 05:13:40.827776    1346 ssh_runner.go:195] Run: systemctl --version
I0806 05:13:41.768350    1346 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.086195718s)
I0806 05:13:41.768536    1346 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0806 05:13:41.776856    1346 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0806 05:13:41.810020    1346 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0806 05:13:41.810156    1346 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0806 05:13:41.824264    1346 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0806 05:13:41.824302    1346 start.go:495] detecting cgroup driver to use...
I0806 05:13:41.824387    1346 detect.go:190] detected "systemd" cgroup driver on host os
I0806 05:13:41.824646    1346 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0806 05:13:41.849501    1346 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0806 05:13:41.864597    1346 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0806 05:13:41.878695    1346 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0806 05:13:41.878746    1346 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0806 05:13:41.893641    1346 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0806 05:13:41.906654    1346 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0806 05:13:41.920074    1346 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0806 05:13:41.932892    1346 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0806 05:13:41.944556    1346 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0806 05:13:41.957085    1346 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0806 05:13:41.973049    1346 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0806 05:13:41.989609    1346 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0806 05:13:42.003029    1346 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0806 05:13:42.015573    1346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0806 05:13:42.096939    1346 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0806 05:13:42.313178    1346 start.go:495] detecting cgroup driver to use...
I0806 05:13:42.313243    1346 detect.go:190] detected "systemd" cgroup driver on host os
I0806 05:13:42.313420    1346 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0806 05:13:42.344224    1346 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0806 05:13:42.344315    1346 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0806 05:13:42.389081    1346 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0806 05:13:42.433667    1346 ssh_runner.go:195] Run: which cri-dockerd
I0806 05:13:42.442611    1346 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0806 05:13:42.461253    1346 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0806 05:13:42.486209    1346 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0806 05:13:42.623770    1346 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0806 05:13:42.731399    1346 docker.go:587] configuring docker to use "systemd" as cgroup driver...
I0806 05:13:42.731636    1346 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0806 05:13:42.756837    1346 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0806 05:13:42.781047    1346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0806 05:13:42.908726    1346 ssh_runner.go:195] Run: sudo systemctl restart docker
I0806 05:13:46.180560    1346 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.271655934s)
I0806 05:13:46.180736    1346 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0806 05:13:46.203944    1346 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0806 05:13:46.226017    1346 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0806 05:13:46.251297    1346 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0806 05:13:46.389945    1346 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0806 05:13:46.488759    1346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0806 05:13:46.639317    1346 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0806 05:13:46.680153    1346 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0806 05:13:46.704536    1346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0806 05:13:46.840198    1346 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0806 05:13:47.343818    1346 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0806 05:13:47.360585    1346 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0806 05:13:47.360642    1346 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0806 05:13:47.365322    1346 start.go:563] Will wait 60s for crictl version
I0806 05:13:47.365397    1346 ssh_runner.go:195] Run: which crictl
I0806 05:13:47.369578    1346 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0806 05:13:47.558808    1346 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0806 05:13:47.558904    1346 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0806 05:13:47.722047    1346 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0806 05:13:47.766577    1346 out.go:235] üê≥  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0806 05:13:47.767680    1346 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0806 05:13:47.797543    1346 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I0806 05:13:47.804028    1346 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.58.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0806 05:13:47.818728    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0806 05:13:47.839204    1346 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.47 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/faizan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0806 05:13:47.839564    1346 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0806 05:13:47.839630    1346 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0806 05:13:47.864812    1346 docker.go:702] Got preloaded images: -- stdout --
quay.io/jetstack/cert-manager-acmesolver:v1.18.2
quay.io/jetstack/cert-manager-controller:v1.18.2
quay.io/jetstack/cert-manager-cainjector:v1.18.2
quay.io/jetstack/cert-manager-webhook:v1.18.2
quay.io/argoproj/argocd:v3.0.6
postgres:15
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.21-0
redis:7.2.7-alpine
registry.k8s.io/coredns/coredns:v1.12.0
ghcr.io/dexidp/dex:v2.41.1
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0806 05:13:47.864821    1346 docker.go:632] Images already preloaded, skipping extraction
I0806 05:13:47.864874    1346 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0806 05:13:47.890658    1346 docker.go:702] Got preloaded images: -- stdout --
quay.io/jetstack/cert-manager-acmesolver:v1.18.2
quay.io/jetstack/cert-manager-cainjector:v1.18.2
quay.io/jetstack/cert-manager-controller:v1.18.2
quay.io/jetstack/cert-manager-webhook:v1.18.2
quay.io/argoproj/argocd:v3.0.6
postgres:15
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.21-0
redis:7.2.7-alpine
registry.k8s.io/coredns/coredns:v1.12.0
ghcr.io/dexidp/dex:v2.41.1
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0806 05:13:47.890693    1346 cache_images.go:84] Images are preloaded, skipping loading
I0806 05:13:47.890703    1346 kubeadm.go:926] updating node { 192.168.58.2 8443 v1.33.1 docker true true} ...
I0806 05:13:47.890910    1346 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0806 05:13:47.890993    1346 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0806 05:13:48.194806    1346 cni.go:84] Creating CNI manager for ""
I0806 05:13:48.194836    1346 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0806 05:13:48.194854    1346 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0806 05:13:48.194944    1346 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0806 05:13:48.195399    1346 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.58.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0806 05:13:48.195589    1346 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0806 05:13:48.216881    1346 binaries.go:44] Found k8s binaries, skipping transfer
I0806 05:13:48.216965    1346 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0806 05:13:48.236694    1346 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0806 05:13:48.274469    1346 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0806 05:13:48.309391    1346 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0806 05:13:48.357623    1346 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0806 05:13:48.366402    1346 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0806 05:13:48.391354    1346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0806 05:13:48.541396    1346 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0806 05:13:48.593922    1346 certs.go:68] Setting up /home/faizan/.minikube/profiles/minikube for IP: 192.168.58.2
I0806 05:13:48.593937    1346 certs.go:194] generating shared ca certs ...
I0806 05:13:48.593963    1346 certs.go:226] acquiring lock for ca certs: {Name:mke255ecd7801f76e2d39d87afe14e3859a2f8aa Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0806 05:13:48.598018    1346 certs.go:235] skipping valid "minikubeCA" ca cert: /home/faizan/.minikube/ca.key
I0806 05:13:48.598601    1346 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/faizan/.minikube/proxy-client-ca.key
I0806 05:13:48.598617    1346 certs.go:256] generating profile certs ...
I0806 05:13:48.599694    1346 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/faizan/.minikube/profiles/minikube/client.key
I0806 05:13:48.600313    1346 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/faizan/.minikube/profiles/minikube/apiserver.key.502bbb95
I0806 05:13:48.601069    1346 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/faizan/.minikube/profiles/minikube/proxy-client.key
I0806 05:13:48.601301    1346 certs.go:484] found cert: /home/faizan/.minikube/certs/ca-key.pem (1675 bytes)
I0806 05:13:48.601434    1346 certs.go:484] found cert: /home/faizan/.minikube/certs/ca.pem (1078 bytes)
I0806 05:13:48.601469    1346 certs.go:484] found cert: /home/faizan/.minikube/certs/cert.pem (1123 bytes)
I0806 05:13:48.601497    1346 certs.go:484] found cert: /home/faizan/.minikube/certs/key.pem (1675 bytes)
I0806 05:13:48.605050    1346 ssh_runner.go:362] scp /home/faizan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0806 05:13:48.668662    1346 ssh_runner.go:362] scp /home/faizan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0806 05:13:48.724893    1346 ssh_runner.go:362] scp /home/faizan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0806 05:13:48.771604    1346 ssh_runner.go:362] scp /home/faizan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0806 05:13:48.814857    1346 ssh_runner.go:362] scp /home/faizan/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0806 05:13:48.859061    1346 ssh_runner.go:362] scp /home/faizan/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0806 05:13:48.897711    1346 ssh_runner.go:362] scp /home/faizan/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0806 05:13:48.930905    1346 ssh_runner.go:362] scp /home/faizan/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0806 05:13:48.964698    1346 ssh_runner.go:362] scp /home/faizan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0806 05:13:48.999921    1346 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0806 05:13:49.025200    1346 ssh_runner.go:195] Run: openssl version
I0806 05:13:49.038319    1346 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0806 05:13:49.051601    1346 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0806 05:13:49.061145    1346 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul 25 06:07 /usr/share/ca-certificates/minikubeCA.pem
I0806 05:13:49.061201    1346 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0806 05:13:49.075048    1346 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0806 05:13:49.096712    1346 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0806 05:13:49.107524    1346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0806 05:13:49.123127    1346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0806 05:13:49.134499    1346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0806 05:13:49.145601    1346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0806 05:13:49.160126    1346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0806 05:13:49.171041    1346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0806 05:13:49.186695    1346 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.47 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/faizan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0806 05:13:49.186806    1346 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0806 05:13:49.220287    1346 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0806 05:13:49.234615    1346 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0806 05:13:49.234625    1346 kubeadm.go:589] restartPrimaryControlPlane start ...
I0806 05:13:49.234678    1346 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0806 05:13:49.252501    1346 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0806 05:13:49.252568    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0806 05:13:49.283058    1346 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:32776"
I0806 05:13:49.283607    1346 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:32776, want: 127.0.0.1:32771
I0806 05:13:49.284108    1346 kubeconfig.go:62] /home/faizan/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I0806 05:13:49.284863    1346 lock.go:35] WriteFile acquiring /home/faizan/.kube/config: {Name:mk5ac4a937f573d4e93034e492f96a34455a71bf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0806 05:13:49.302709    1346 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0806 05:13:49.319238    1346 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0806 05:13:49.319261    1346 kubeadm.go:593] duration metric: took 84.631667ms to restartPrimaryControlPlane
I0806 05:13:49.319268    1346 kubeadm.go:394] duration metric: took 132.614598ms to StartCluster
I0806 05:13:49.319362    1346 settings.go:142] acquiring lock: {Name:mk4a4a936d1be1b658f172765c3b68612790e3cb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0806 05:13:49.319620    1346 settings.go:150] Updating kubeconfig:  /home/faizan/.kube/config
I0806 05:13:49.320209    1346 lock.go:35] WriteFile acquiring /home/faizan/.kube/config: {Name:mk5ac4a937f573d4e93034e492f96a34455a71bf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0806 05:13:49.320540    1346 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0806 05:13:49.321008    1346 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0806 05:13:49.321107    1346 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0806 05:13:49.321125    1346 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0806 05:13:49.321132    1346 addons.go:247] addon storage-provisioner should already be in state true
I0806 05:13:49.321166    1346 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0806 05:13:49.321194    1346 host.go:66] Checking if "minikube" exists ...
I0806 05:13:49.321189    1346 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0806 05:13:49.321229    1346 addons.go:69] Setting ingress=true in profile "minikube"
I0806 05:13:49.321253    1346 addons.go:238] Setting addon ingress=true in "minikube"
W0806 05:13:49.321261    1346 addons.go:247] addon ingress should already be in state true
I0806 05:13:49.321299    1346 host.go:66] Checking if "minikube" exists ...
I0806 05:13:49.321301    1346 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0806 05:13:49.321368    1346 addons.go:69] Setting dashboard=true in profile "minikube"
I0806 05:13:49.321380    1346 addons.go:238] Setting addon dashboard=true in "minikube"
W0806 05:13:49.321387    1346 addons.go:247] addon dashboard should already be in state true
I0806 05:13:49.321410    1346 host.go:66] Checking if "minikube" exists ...
I0806 05:13:49.321893    1346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0806 05:13:49.321898    1346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0806 05:13:49.322289    1346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0806 05:13:49.322339    1346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0806 05:13:49.329265    1346 out.go:177] üîé  Verifying Kubernetes components...
I0806 05:13:49.335599    1346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0806 05:13:49.364978    1346 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0806 05:13:49.364994    1346 addons.go:247] addon default-storageclass should already be in state true
I0806 05:13:49.365024    1346 host.go:66] Checking if "minikube" exists ...
I0806 05:13:49.365550    1346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0806 05:13:49.367145    1346 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0806 05:13:49.370511    1346 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0806 05:13:49.372260    1346 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/controller:v1.12.2
I0806 05:13:49.377723    1346 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0806 05:13:49.377737    1346 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0806 05:13:49.377800    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:49.380013    1346 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0806 05:13:49.384883    1346 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0806 05:13:49.384903    1346 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0806 05:13:49.384993    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:49.385160    1346 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3
I0806 05:13:49.389812    1346 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3
I0806 05:13:49.398338    1346 addons.go:435] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0806 05:13:49.398371    1346 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16078 bytes)
I0806 05:13:49.398455    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:49.415074    1346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/faizan/.minikube/machines/minikube/id_rsa Username:docker}
I0806 05:13:49.420767    1346 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0806 05:13:49.420785    1346 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0806 05:13:49.420856    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0806 05:13:49.439948    1346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/faizan/.minikube/machines/minikube/id_rsa Username:docker}
I0806 05:13:49.447853    1346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/faizan/.minikube/machines/minikube/id_rsa Username:docker}
I0806 05:13:49.453072    1346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/faizan/.minikube/machines/minikube/id_rsa Username:docker}
I0806 05:13:49.520144    1346 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0806 05:13:49.548243    1346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0806 05:13:49.576188    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0806 05:13:49.586762    1346 api_server.go:52] waiting for apiserver process to appear ...
I0806 05:13:49.586855    1346 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0806 05:13:49.601505    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0806 05:13:49.603461    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0806 05:13:49.609110    1346 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0806 05:13:49.609294    1346 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0806 05:13:49.650618    1346 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0806 05:13:49.650658    1346 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0806 05:13:49.702217    1346 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0806 05:13:49.702233    1346 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0806 05:13:49.747629    1346 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0806 05:13:49.747710    1346 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0806 05:13:49.803222    1346 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0806 05:13:49.803243    1346 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0806 05:13:49.838284    1346 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0806 05:13:49.838302    1346 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0806 05:13:49.906264    1346 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0806 05:13:49.906280    1346 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
W0806 05:13:49.912867    1346 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:49.912961    1346 retry.go:31] will retry after 146.118169ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0806 05:13:49.913172    1346 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:49.913210    1346 retry.go:31] will retry after 331.255459ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0806 05:13:49.913696    1346 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:49.913710    1346 retry.go:31] will retry after 183.130071ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:49.942787    1346 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0806 05:13:49.942801    1346 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0806 05:13:49.980124    1346 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0806 05:13:49.980142    1346 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0806 05:13:50.017778    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0806 05:13:50.059468    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0806 05:13:50.088034    1346 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0806 05:13:50.097174    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0806 05:13:50.133574    1346 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:50.133601    1346 retry.go:31] will retry after 192.609353ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0806 05:13:50.190852    1346 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:50.190875    1346 retry.go:31] will retry after 329.967446ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0806 05:13:50.226461    1346 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:50.226486    1346 retry.go:31] will retry after 358.261514ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:50.244819    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0806 05:13:50.327073    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0806 05:13:50.358207    1346 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:50.358232    1346 retry.go:31] will retry after 548.183896ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0806 05:13:50.431409    1346 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:50.431483    1346 retry.go:31] will retry after 387.682921ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:50.521614    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0806 05:13:50.585934    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0806 05:13:50.586855    1346 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0806 05:13:50.646343    1346 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:50.646365    1346 retry.go:31] will retry after 596.630916ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0806 05:13:50.710633    1346 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:50.710659    1346 retry.go:31] will retry after 394.933476ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0806 05:13:50.710694    1346 api_server.go:72] duration metric: took 1.390125925s to wait for apiserver process to appear ...
I0806 05:13:50.710704    1346 api_server.go:88] waiting for apiserver healthz status ...
I0806 05:13:50.710722    1346 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0806 05:13:50.819805    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0806 05:13:50.907148    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0806 05:13:51.106421    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0806 05:13:51.243718    1346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0806 05:13:52.420862    1346 api_server.go:279] https://127.0.0.1:32771/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0806 05:13:52.420903    1346 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0806 05:13:52.420924    1346 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0806 05:13:52.485133    1346 api_server.go:279] https://127.0.0.1:32771/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0806 05:13:52.485153    1346 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0806 05:13:52.711256    1346 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0806 05:13:52.719784    1346 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0806 05:13:52.719812    1346 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0806 05:13:53.216070    1346 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0806 05:13:53.229730    1346 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0806 05:13:53.229752    1346 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0806 05:13:53.711325    1346 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0806 05:13:53.731474    1346 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0806 05:13:53.731494    1346 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0806 05:13:54.211824    1346 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0806 05:13:54.226412    1346 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0806 05:13:54.226470    1346 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0806 05:13:54.712056    1346 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0806 05:13:54.745129    1346 api_server.go:279] https://127.0.0.1:32771/healthz returned 200:
ok
I0806 05:13:54.758667    1346 api_server.go:141] control plane version: v1.33.1
I0806 05:13:54.758692    1346 api_server.go:131] duration metric: took 4.047981579s to wait for apiserver health ...
I0806 05:13:54.758805    1346 system_pods.go:43] waiting for kube-system pods to appear ...
I0806 05:13:54.780590    1346 system_pods.go:59] 8 kube-system pods found
I0806 05:13:54.780632    1346 system_pods.go:61] "coredns-674b8bbfcf-hhn6w" [423ad4cf-3201-486d-ba34-6ec42b89e754] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0806 05:13:54.780641    1346 system_pods.go:61] "coredns-674b8bbfcf-p8cgl" [faac8882-fd1c-4745-a11b-3a742c54e792] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0806 05:13:54.780651    1346 system_pods.go:61] "etcd-minikube" [c88b6b2f-3467-477c-be26-6c21b5edc996] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0806 05:13:54.780660    1346 system_pods.go:61] "kube-apiserver-minikube" [d345477d-f1af-4bd0-8983-cd277809f431] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0806 05:13:54.780667    1346 system_pods.go:61] "kube-controller-manager-minikube" [d8cc1d8e-6be8-42a0-a020-b2464f3abbad] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0806 05:13:54.780672    1346 system_pods.go:61] "kube-proxy-s7bwv" [20a50739-725e-40dd-b21d-f8da12696c31] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0806 05:13:54.780678    1346 system_pods.go:61] "kube-scheduler-minikube" [b9f206f9-d406-4996-8a5e-90b40eedec9d] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0806 05:13:54.780686    1346 system_pods.go:61] "storage-provisioner" [5bfc2dda-03bd-4c08-9973-122b34df3832] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0806 05:13:54.780695    1346 system_pods.go:74] duration metric: took 21.86412ms to wait for pod list to return data ...
I0806 05:13:54.780708    1346 kubeadm.go:578] duration metric: took 5.4601433s to wait for: map[apiserver:true system_pods:true]
I0806 05:13:54.780722    1346 node_conditions.go:102] verifying NodePressure condition ...
I0806 05:13:54.784209    1346 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0806 05:13:54.784236    1346 node_conditions.go:123] node cpu capacity is 8
I0806 05:13:54.784298    1346 node_conditions.go:105] duration metric: took 3.570693ms to run NodePressure ...
I0806 05:13:54.784313    1346 start.go:241] waiting for startup goroutines ...
I0806 05:13:56.011141    1346 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (5.191293567s)
I0806 05:13:56.015200    1346 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0806 05:13:56.590084    1346 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (5.483635013s)
I0806 05:13:56.590419    1346 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: (5.682699706s)
I0806 05:13:56.590445    1346 addons.go:479] Verifying addon ingress=true in "minikube"
I0806 05:13:56.590442    1346 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.346701511s)
I0806 05:13:56.593986    1346 out.go:177] üîé  Verifying ingress addon...
I0806 05:13:56.598921    1346 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0806 05:13:56.605278    1346 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0806 05:13:56.605299    1346 kapi.go:107] duration metric: took 6.386556ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0806 05:13:56.613223    1346 out.go:177] üåü  Enabled addons: dashboard, storage-provisioner, ingress, default-storageclass
I0806 05:13:56.618128    1346 addons.go:514] duration metric: took 7.297360196s for enable addons: enabled=[dashboard storage-provisioner ingress default-storageclass]
I0806 05:13:56.618177    1346 start.go:246] waiting for cluster config update ...
I0806 05:13:56.618192    1346 start.go:255] writing updated cluster config ...
I0806 05:13:56.618600    1346 ssh_runner.go:195] Run: rm -f paused
I0806 05:13:57.012515    1346 start.go:607] kubectl: 1.33.3, cluster: 1.33.1 (minor skew: 0)
I0806 05:13:57.015027    1346 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Aug 06 05:13:50 minikube cri-dockerd[1465]: time="2025-08-06T05:13:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-hx2jl_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"221bc614674155937c4e8fd3d57fb963b83104a4ca6da244ccc25804d654e2be\""
Aug 06 05:13:50 minikube cri-dockerd[1465]: time="2025-08-06T05:13:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-dex-server-7d9dfb4fb8-kblv8_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fda08542f8f6fe218f2cf3f15963d0894e8550ac40b36ab220f79988aca6deb6\""
Aug 06 05:13:50 minikube cri-dockerd[1465]: time="2025-08-06T05:13:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-redis-656c79549c-2cz8v_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"090fa983b2a80b37679f7a67006c25f827cc2f785b5447eeca956c71e6286048\""
Aug 06 05:13:50 minikube cri-dockerd[1465]: time="2025-08-06T05:13:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-hhn6w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"7bb52f5e9985b933c92a4e4db20211b29703a80f40ce3813134d76b17267e4b7\""
Aug 06 05:13:50 minikube cri-dockerd[1465]: time="2025-08-06T05:13:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-7779f9b69b-x994z_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5fe8fce0950ee34b637dba212febf49681a63b49852295be1db2f7233fb8a429\""
Aug 06 05:13:50 minikube cri-dockerd[1465]: time="2025-08-06T05:13:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"cert-manager-69f748766f-spgbb_cert-manager\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"7f1ff06f85dd9c650c6d2ab26f3e07ff149c70da1f27f11784466dca70ca550b\""
Aug 06 05:13:50 minikube cri-dockerd[1465]: time="2025-08-06T05:13:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-67c5cb88f-5xw4j_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"62a04be16abdf609235c7973acab64ae01eeedc33c909ecff4aa74f772e3da9f\""
Aug 06 05:13:51 minikube cri-dockerd[1465]: time="2025-08-06T05:13:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-67c5cb88f-5xw4j_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e749a1bb01b948eabc0b05ca16741e735cb8c5f013648c161ef5c38070cc6429\""
Aug 06 05:13:51 minikube cri-dockerd[1465]: time="2025-08-06T05:13:51Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-applicationset-controller-655cc58ff8-2rb9p_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6101dbb6c642af6192c17163bfbc73429e7714330e0c2dd1de4c3d6d654ed9dd\""
Aug 06 05:13:51 minikube cri-dockerd[1465]: time="2025-08-06T05:13:51Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-repo-server-856b768fd9-5p2t7_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b659e4117bf94aa39185d57b21292e620dc9af8c2250490ec9332768264771ad\""
Aug 06 05:13:51 minikube cri-dockerd[1465]: time="2025-08-06T05:13:51Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"cert-manager-cainjector-7cf6557c49-zhjw8_cert-manager\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3f197709f5580e968093c5a63a66f9af792ea8b62c52d9dd214de3ab6664088b\""
Aug 06 05:13:52 minikube cri-dockerd[1465]: time="2025-08-06T05:13:52Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/73f076a250db2d5ab582846810ed3818e8341f72f12bbf4194641e6004050ccc/resolv.conf as [nameserver 192.168.58.1 options ndots:0]"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/041c932dd60b2ac15bc4111a6d30a705271d2689aa3773bd449768d081e74fee/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f7ef17f28de7fb89a77cfe12b3f00e5f4d749e48bde52f81a22c3f845612915a/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/162316d9ce87c084c514a86ae3ca8783d7b5bac4edcce0cd740dcdaf9b76385f/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0e9a19717fd674136c04c37d9d14fa95b0b43ba6568e654bc2db165b77add07e/resolv.conf as [nameserver 192.168.58.1 options ndots:0]"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c118747d8bc3676b733aad08fafe3ddce8ed74b71a752ff1a068f713e417c78e/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3e9269c024dba5acef3241e0cd26b3dba42e928496ea5c0a99859fa7d3179dac/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b3644cc201ecccb88c36c8def9bc25076f43bafb7c3683c815151fe021134690/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d27bf53f934dc6065287019c1d224317455f37b5b13c844a8df953d4c53c47cd/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cd73d722da23f4b580aa0e541f7253eaa86cab64e6f3f88ef500f5cb1a790844/resolv.conf as [nameserver 192.168.58.1 options ndots:0]"
Aug 06 05:13:54 minikube cri-dockerd[1465]: time="2025-08-06T05:13:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5afe935437527a9b7b57d84d9f10f9731fffbcee414e9db789ea84db3d944689/resolv.conf as [nameserver 192.168.58.1 options ndots:0]"
Aug 06 05:13:55 minikube cri-dockerd[1465]: time="2025-08-06T05:13:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e24c26fc87ef0b26575e876f7117d6846c09fcbb403c9695732fa8993ce2a46c/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:55 minikube cri-dockerd[1465]: time="2025-08-06T05:13:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2ac0b822ebb4f2ace3cdebfb4d2a635c9758f8f3a399b282833b89c281ccaf0f/resolv.conf as [nameserver 10.96.0.10 search cert-manager.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:55 minikube cri-dockerd[1465]: time="2025-08-06T05:13:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e4d999985445cbe7d49cf38c983943dc963abbf3c27ac086c49384cd63858878/resolv.conf as [nameserver 10.96.0.10 search cert-manager.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:55 minikube cri-dockerd[1465]: time="2025-08-06T05:13:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/58c38608ea0a77bcb27dba1ff39c21eeb636f12efa632b60fa8932d8fada58c3/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:55 minikube cri-dockerd[1465]: time="2025-08-06T05:13:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d2cec3175711284202114a11e7a7f66f3642b71048cd2ced7e510929c746621f/resolv.conf as [nameserver 10.96.0.10 search cert-manager.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:55 minikube cri-dockerd[1465]: time="2025-08-06T05:13:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a2282d7590bc13f8ff8a4f17a7ba9cfe77a9a224583f2baec021d48d76d9d2e7/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:55 minikube cri-dockerd[1465]: time="2025-08-06T05:13:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/785210e0c7ff1274212b0a00da75f7e27567efe7d8eebcae788aa71222743768/resolv.conf as [nameserver 10.96.0.10 search cert-manager.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:13:56 minikube dockerd[1117]: time="2025-08-06T05:13:56.524830549Z" level=info msg="ignoring event" container=d49cd6e7282e5f348bcdd64c5e5f881935a0325a2206012a55b12767eb30840a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 06 05:13:56 minikube dockerd[1117]: time="2025-08-06T05:13:56.686656286Z" level=info msg="ignoring event" container=9b7c8101c48f6595f79dac603f41b5a22a1f1e2ec4807b5fed6972d9e12b6834 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 06 05:13:57 minikube dockerd[1117]: time="2025-08-06T05:13:57.040284287Z" level=info msg="ignoring event" container=8cd8d7199f2de944e265923f814cbf3e429a875eb855ea5ba1c0d5d4681ba3cb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 06 05:13:58 minikube cri-dockerd[1465]: time="2025-08-06T05:13:58Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v3.0.6: Status: Image is up to date for quay.io/argoproj/argocd:v3.0.6"
Aug 06 05:14:01 minikube cri-dockerd[1465]: time="2025-08-06T05:14:01Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v3.0.6: Status: Image is up to date for quay.io/argoproj/argocd:v3.0.6"
Aug 06 05:14:03 minikube cri-dockerd[1465]: time="2025-08-06T05:14:03Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v3.0.6: Status: Image is up to date for quay.io/argoproj/argocd:v3.0.6"
Aug 06 05:14:05 minikube dockerd[1117]: time="2025-08-06T05:14:05.801458624Z" level=info msg="ignoring event" container=975941362e96481f9eb07b94a1f0b902107cc3b3b207b136be7f569e24f49a78 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 06 05:14:05 minikube dockerd[1117]: time="2025-08-06T05:14:05.810538453Z" level=info msg="ignoring event" container=adacaee3a3625b635859202a966a705d9506a8de786672ba9f36c0470d94b229 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 06 05:14:06 minikube cri-dockerd[1465]: time="2025-08-06T05:14:06Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v3.0.6: Status: Image is up to date for quay.io/argoproj/argocd:v3.0.6"
Aug 06 05:14:06 minikube dockerd[1117]: time="2025-08-06T05:14:06.813950118Z" level=info msg="ignoring event" container=7f99922a1fa8c4569f0fb85c81a42fa3117e147ae8eed152a32eff4a3ac07c73 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 06 05:14:07 minikube dockerd[1117]: time="2025-08-06T05:14:07.016568899Z" level=info msg="ignoring event" container=25dfcd878e755332677315f5bc383b70d70259f12d555676656d65cee3da8c56 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 06 05:14:09 minikube cri-dockerd[1465]: time="2025-08-06T05:14:09Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v3.0.6: Status: Image is up to date for quay.io/argoproj/argocd:v3.0.6"
Aug 06 05:14:13 minikube cri-dockerd[1465]: time="2025-08-06T05:14:13Z" level=info msg="Stop pulling image redis:7.2.7-alpine: Status: Image is up to date for redis:7.2.7-alpine"
Aug 06 05:14:17 minikube cri-dockerd[1465]: time="2025-08-06T05:14:17Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v3.0.6: Status: Image is up to date for quay.io/argoproj/argocd:v3.0.6"
Aug 06 05:14:18 minikube cri-dockerd[1465]: time="2025-08-06T05:14:18Z" level=info msg="Stop pulling image ghcr.io/dexidp/dex:v2.41.1: Status: Image is up to date for ghcr.io/dexidp/dex:v2.41.1"
Aug 06 05:14:20 minikube dockerd[1117]: time="2025-08-06T05:14:20.218147182Z" level=info msg="ignoring event" container=bd5b8cb3ea1b8548858486874d2fc9e004952b19412a2c648fd9691155833b71 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 06 05:14:49 minikube dockerd[1117]: time="2025-08-06T05:14:49.631130945Z" level=info msg="ignoring event" container=ed9e5ab319fe269b74ea6e53b23be71616465842b2244708974ffd15b508cd72 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 06 05:15:37 minikube dockerd[1117]: time="2025-08-06T05:15:37.250212657Z" level=info msg="ignoring event" container=ce8788d929680bed80b71fada7ee86eaa56135ba78be87417cdcae87f49928e9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 06 05:15:56 minikube cri-dockerd[1465]: time="2025-08-06T05:15:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d2ab5179ef875684df38fcc20af2085da5676159fc1f1a65345a713cf7476471/resolv.conf as [nameserver 10.96.0.10 search dev.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:15:56 minikube cri-dockerd[1465]: time="2025-08-06T05:15:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bd318ab8d3e996bf5d2c633cdd611b1831b161a902c230ccfe2cd73ac50797ff/resolv.conf as [nameserver 10.96.0.10 search dev.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 06 05:15:59 minikube dockerd[1117]: time="2025-08-06T05:15:59.279383045Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 06 05:15:59 minikube dockerd[1117]: time="2025-08-06T05:15:59.279481929Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 06 05:16:02 minikube dockerd[1117]: time="2025-08-06T05:16:02.024251198Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 06 05:16:02 minikube dockerd[1117]: time="2025-08-06T05:16:02.024491921Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 06 05:16:17 minikube dockerd[1117]: time="2025-08-06T05:16:17.326916248Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 06 05:16:17 minikube dockerd[1117]: time="2025-08-06T05:16:17.327053421Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 06 05:16:20 minikube dockerd[1117]: time="2025-08-06T05:16:20.589450441Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 06 05:16:20 minikube dockerd[1117]: time="2025-08-06T05:16:20.589569182Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 06 05:16:45 minikube dockerd[1117]: time="2025-08-06T05:16:45.666272336Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 06 05:16:45 minikube dockerd[1117]: time="2025-08-06T05:16:45.666379858Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED              STATE               NAME                               ATTEMPT             POD ID              POD
ce8788d929680       0b9b5651e43cf                                                                                                                About a minute ago   Exited              cert-manager-controller            14                  e4d999985445c       cert-manager-8555c66c68-4tpgq
c16fd7e4d8c1f       c9dc06fbdcc15                                                                                                                2 minutes ago        Running             cert-manager-cainjector            4                   d2cec31757112       cert-manager-cainjector-7cf6557c49-zhjw8
6fde69e8e7b3f       6e38f40d628db                                                                                                                2 minutes ago        Running             storage-provisioner                26                  5afe935437527       storage-provisioner
3fd2a36d2f6bc       ghcr.io/dexidp/dex@sha256:bc7cfce7c17f52864e2bb2a4dc1d2f86a41e3019f6d42e81d92a301fad0c8a1d                                   2 minutes ago        Running             dex                                2                   b3644cc201ecc       argocd-dex-server-7d9dfb4fb8-kblv8
674c5ec49b3f2       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              2 minutes ago        Running             argocd-repo-server                 2                   58c38608ea0a7       argocd-repo-server-856b768fd9-5p2t7
4f3cdb66deb26       07655ddf2eebe                                                                                                                2 minutes ago        Running             kubernetes-dashboard               16                  041c932dd60b2       kubernetes-dashboard-7779f9b69b-x994z
15141a6cbf70d       redis@sha256:ddd16a9b1575a774c7e62956be8daa1de5b32cfb5c25b7a216aefed8e0919f9b                                                2 minutes ago        Running             redis                              2                   a2282d7590bc1       argocd-redis-656c79549c-2cz8v
de7191679b7bd       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              2 minutes ago        Running             argocd-applicationset-controller   2                   d27bf53f934dc       argocd-applicationset-controller-655cc58ff8-2rb9p
7f99922a1fa8c       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              2 minutes ago        Exited              copyutil                           2                   b3644cc201ecc       argocd-dex-server-7d9dfb4fb8-kblv8
11c2ce0a816d8       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              2 minutes ago        Running             argocd-notifications-controller    2                   3e9269c024dba       argocd-notifications-controller-6c6848bc4c-cg7jf
fc12343fb1711       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              2 minutes ago        Running             argocd-application-controller      2                   c118747d8bc36       argocd-application-controller-0
a56981565493d       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              2 minutes ago        Running             argocd-server                      2                   f7ef17f28de7f       argocd-server-99c485944-hvqdz
4e7223eb8fe19       3d5c6a600b312                                                                                                                2 minutes ago        Running             controller                         2                   e24c26fc87ef0       ingress-nginx-controller-67c5cb88f-5xw4j
25dfcd878e755       c9dc06fbdcc15                                                                                                                2 minutes ago        Exited              cert-manager-cainjector            3                   d2cec31757112       cert-manager-cainjector-7cf6557c49-zhjw8
c26688f6905e2       8f2c78201d80e                                                                                                                2 minutes ago        Running             cert-manager-webhook               2                   785210e0c7ff1       cert-manager-webhook-58f4cff74d-kzcgz
8cd8d7199f2de       41f4fb4b71507                                                                                                                2 minutes ago        Exited              secret-init                        2                   a2282d7590bc1       argocd-redis-656c79549c-2cz8v
d49cd6e7282e5       41f4fb4b71507                                                                                                                2 minutes ago        Exited              copyutil                           2                   58c38608ea0a7       argocd-repo-server-856b768fd9-5p2t7
bd14a847455bb       0b9b5651e43cf                                                                                                                2 minutes ago        Running             cert-manager-controller            2                   2ac0b822ebb4f       cert-manager-69f748766f-spgbb
6505e100c921e       1cf5f116067c6                                                                                                                2 minutes ago        Running             coredns                            13                  0e9a19717fd67       coredns-674b8bbfcf-hhn6w
472f7a5756f4f       115053965e86b                                                                                                                2 minutes ago        Running             dashboard-metrics-scraper          8                   162316d9ce87c       dashboard-metrics-scraper-5d59dccf9b-hx2jl
d9baed162aa5d       b79c189b052cd                                                                                                                2 minutes ago        Running             kube-proxy                         13                  cd73d722da23f       kube-proxy-s7bwv
975941362e964       6e38f40d628db                                                                                                                2 minutes ago        Exited              storage-provisioner                25                  5afe935437527       storage-provisioner
adacaee3a3625       07655ddf2eebe                                                                                                                2 minutes ago        Exited              kubernetes-dashboard               15                  041c932dd60b2       kubernetes-dashboard-7779f9b69b-x994z
08016939c5036       1cf5f116067c6                                                                                                                2 minutes ago        Running             coredns                            13                  73f076a250db2       coredns-674b8bbfcf-p8cgl
162da276ef8d0       398c985c0d950                                                                                                                2 minutes ago        Running             kube-scheduler                     13                  da8a0208100ed       kube-scheduler-minikube
8abdae0e8fb3e       c6ab243b29f82                                                                                                                2 minutes ago        Running             kube-apiserver                     13                  92c150a983085       kube-apiserver-minikube
d7fedce18f6a7       499038711c081                                                                                                                2 minutes ago        Running             etcd                               13                  7ffa3b4e2b150       etcd-minikube
4f566a7ffba75       ef43894fa110c                                                                                                                2 minutes ago        Running             kube-controller-manager            13                  db0a14b3c5e4f       kube-controller-manager-minikube
f350f5226e450       ghcr.io/dexidp/dex@sha256:bc7cfce7c17f52864e2bb2a4dc1d2f86a41e3019f6d42e81d92a301fad0c8a1d                                   16 hours ago         Exited              dex                                1                   fda08542f8f6f       argocd-dex-server-7d9dfb4fb8-kblv8
f2b708391f4a4       redis@sha256:ddd16a9b1575a774c7e62956be8daa1de5b32cfb5c25b7a216aefed8e0919f9b                                                16 hours ago         Exited              redis                              1                   090fa983b2a80       argocd-redis-656c79549c-2cz8v
7c4cff6e0a1b5       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              16 hours ago         Exited              argocd-repo-server                 1                   b659e4117bf94       argocd-repo-server-856b768fd9-5p2t7
c9a87c9e60ca7       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              16 hours ago         Exited              argocd-application-controller      1                   b9c7f00cc98fd       argocd-application-controller-0
ff27e7ffa7283       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              16 hours ago         Exited              argocd-notifications-controller    1                   989c303c407e1       argocd-notifications-controller-6c6848bc4c-cg7jf
0563a9f8ad80d       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              16 hours ago         Exited              argocd-applicationset-controller   1                   6101dbb6c642a       argocd-applicationset-controller-655cc58ff8-2rb9p
85e0067822019       quay.io/argoproj/argocd@sha256:a45307e2695d0fd93713e3d211b71086ac75a85dc8afbb28a249bdc4b3b0b2b9                              16 hours ago         Exited              argocd-server                      1                   ab7e1641b4909       argocd-server-99c485944-hvqdz
3b4906c84f70c       8f2c78201d80e                                                                                                                16 hours ago         Exited              cert-manager-webhook               1                   fda9befb851ac       cert-manager-webhook-58f4cff74d-kzcgz
3d37e6321122c       0b9b5651e43cf                                                                                                                16 hours ago         Exited              cert-manager-controller            1                   7f1ff06f85dd9       cert-manager-69f748766f-spgbb
3153302e896fa       3d5c6a600b312                                                                                                                16 hours ago         Exited              controller                         1                   62a04be16abdf       ingress-nginx-controller-67c5cb88f-5xw4j
47288beec069e       1cf5f116067c6                                                                                                                16 hours ago         Exited              coredns                            12                  7428977fbe3ef       coredns-674b8bbfcf-p8cgl
5c530d8a1d233       1cf5f116067c6                                                                                                                16 hours ago         Exited              coredns                            12                  7bb52f5e9985b       coredns-674b8bbfcf-hhn6w
0bc444405bc66       b79c189b052cd                                                                                                                16 hours ago         Exited              kube-proxy                         12                  253bf0fa1d83e       kube-proxy-s7bwv
ef64463d5e88c       115053965e86b                                                                                                                16 hours ago         Exited              dashboard-metrics-scraper          7                   221bc61467415       dashboard-metrics-scraper-5d59dccf9b-hx2jl
add380676697f       499038711c081                                                                                                                16 hours ago         Exited              etcd                               12                  22f8eccd9e0d7       etcd-minikube
bbc3a3968e040       c6ab243b29f82                                                                                                                16 hours ago         Exited              kube-apiserver                     12                  843423ba9cdd3       kube-apiserver-minikube
178222bfd0a9e       398c985c0d950                                                                                                                16 hours ago         Exited              kube-scheduler                     12                  7e08dfa798325       kube-scheduler-minikube
e2b6b917777a5       ef43894fa110c                                                                                                                16 hours ago         Exited              kube-controller-manager            12                  4a97315c84945       kube-controller-manager-minikube
a8793ba2cc9c1       fcb7220db7030                                                                                                                18 hours ago         Exited              patch                              1                   71c312d96977c       ingress-nginx-admission-patch-lgvbp
dd1cf17523f58       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524   18 hours ago         Exited              create                             0                   66c60aea5713c       ingress-nginx-admission-create-gz4db


==> controller_ingress [3153302e896f] <==
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.12.2
  Build:         7995f327cd0c228bda326a9e287ba559799bffe0
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.25.5

-------------------------------------------------------------------------------

W0805 12:57:48.293781       7 client_config.go:667] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0805 12:57:48.295956       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
W0805 12:57:59.447403       7 main.go:245] Initial connection to the Kubernetes API server was retried 1 times.
I0805 12:57:59.447452       7 main.go:248] "Running in Kubernetes cluster" major="1" minor="33" git="v1.33.1" state="clean" commit="8adc0f041b8e7ad1d30e29cc59c6ae7a15e19828" platform="linux/amd64"
I0805 12:57:59.507668       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0805 12:57:59.537016       7 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0805 12:57:59.554249       7 nginx.go:271] "Starting NGINX Ingress controller"
I0805 12:57:59.564940       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"9975ef8b-24dc-4bba-a330-45bdc42a0e62", APIVersion:"v1", ResourceVersion:"60610", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0805 12:57:59.568175       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"e72e7555-d3dd-4fbd-93f9-e1c777782111", APIVersion:"v1", ResourceVersion:"60611", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0805 12:57:59.568234       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"ad41c7f6-019a-491c-a946-579efef0de95", APIVersion:"v1", ResourceVersion:"60612", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0805 12:58:00.757535       7 nginx.go:317] "Starting NGINX process"
I0805 12:58:00.757655       7 leaderelection.go:257] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0805 12:58:00.759127       7 nginx.go:337] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0805 12:58:00.761137       7 controller.go:196] "Configuration changes detected, backend reload required"
I0805 12:58:00.770421       7 leaderelection.go:271] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0805 12:58:00.770565       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-67c5cb88f-5xw4j"
I0805 12:58:00.774931       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-5xw4j" node="minikube"
I0805 12:58:00.829244       7 controller.go:216] "Backend successfully reloaded"
I0805 12:58:00.829409       7 controller.go:227] "Initial sync, sleeping for 1 second"
I0805 12:58:00.829563       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-67c5cb88f-5xw4j", UID:"94597f86-1ed7-42ad-b9c6-e198006d860e", APIVersion:"v1", ResourceVersion:"65716", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration


==> controller_ingress [4e7223eb8fe1] <==
W0806 05:13:56.549557       7 client_config.go:667] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0806 05:13:56.551146       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
W0806 05:14:08.714712       7 main.go:245] Initial connection to the Kubernetes API server was retried 1 times.
I0806 05:14:08.714855       7 main.go:248] "Running in Kubernetes cluster" major="1" minor="33" git="v1.33.1" state="clean" commit="8adc0f041b8e7ad1d30e29cc59c6ae7a15e19828" platform="linux/amd64"
I0806 05:14:08.776780       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0806 05:14:08.808659       7 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0806 05:14:08.831109       7 nginx.go:271] "Starting NGINX Ingress controller"
I0806 05:14:08.842666       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"9975ef8b-24dc-4bba-a330-45bdc42a0e62", APIVersion:"v1", ResourceVersion:"60610", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0806 05:14:08.849015       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"e72e7555-d3dd-4fbd-93f9-e1c777782111", APIVersion:"v1", ResourceVersion:"60611", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0806 05:14:08.849166       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"ad41c7f6-019a-491c-a946-579efef0de95", APIVersion:"v1", ResourceVersion:"60612", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0806 05:14:10.035479       7 nginx.go:317] "Starting NGINX process"
I0806 05:14:10.035717       7 leaderelection.go:257] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0806 05:14:10.036214       7 nginx.go:337] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0806 05:14:10.045799       7 controller.go:196] "Configuration changes detected, backend reload required"
I0806 05:14:10.054123       7 leaderelection.go:271] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0806 05:14:10.054442       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-67c5cb88f-5xw4j"
I0806 05:14:10.058183       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-5xw4j" node="minikube"
I0806 05:14:10.110966       7 controller.go:216] "Backend successfully reloaded"
I0806 05:14:10.111086       7 controller.go:227] "Initial sync, sleeping for 1 second"
I0806 05:14:10.111131       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-67c5cb88f-5xw4j", UID:"94597f86-1ed7-42ad-b9c6-e198006d860e", APIVersion:"v1", ResourceVersion:"66543", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.12.2
  Build:         7995f327cd0c228bda326a9e287ba559799bffe0
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.25.5

-------------------------------------------------------------------------------



==> coredns [08016939c503] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/errors: 2 argocd-repo-server. A: read udp 10.244.0.180:34577->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. AAAA: read udp 10.244.0.180:39756->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. A: read udp 10.244.0.180:50682->192.168.58.1:53: i/o timeout


==> coredns [47288beec069] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/errors: 2 argocd-repo-server. AAAA: read udp 10.244.0.166:42637->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-redis. AAAA: read udp 10.244.0.166:33517->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-redis. A: read udp 10.244.0.166:36064->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. A: read udp 10.244.0.166:50215->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. AAAA: read udp 10.244.0.166:37690->192.168.58.1:53: i/o timeout


==> coredns [5c530d8a1d23] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/errors: 2 argocd-repo-server. A: read udp 10.244.0.165:41154->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. AAAA: read udp 10.244.0.165:34407->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-redis. AAAA: read udp 10.244.0.165:39065->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-redis. A: read udp 10.244.0.165:37458->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. A: read udp 10.244.0.165:32900->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. AAAA: read udp 10.244.0.165:37330->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. A: read udp 10.244.0.165:37269->192.168.58.1:53: i/o timeout


==> coredns [6505e100c921] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/errors: 2 argocd-repo-server. AAAA: read udp 10.244.0.184:38880->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. A: read udp 10.244.0.184:42098->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-redis. AAAA: read udp 10.244.0.184:53229->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-redis. A: read udp 10.244.0.184:33640->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. A: read udp 10.244.0.184:55160->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. AAAA: read udp 10.244.0.184:37814->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. A: read udp 10.244.0.184:59646->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-repo-server. AAAA: read udp 10.244.0.184:55431->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-redis. AAAA: read udp 10.244.0.184:57988->192.168.58.1:53: i/o timeout
[ERROR] plugin/errors: 2 argocd-redis. A: read udp 10.244.0.184:57625->192.168.58.1:53: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_07_25T06_07_47_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 25 Jul 2025 06:07:44 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 06 Aug 2025 05:16:43 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 06 Aug 2025 05:13:52 +0000   Fri, 25 Jul 2025 06:07:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 06 Aug 2025 05:13:52 +0000   Fri, 25 Jul 2025 06:07:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 06 Aug 2025 05:13:52 +0000   Fri, 25 Jul 2025 06:07:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 06 Aug 2025 05:13:52 +0000   Fri, 25 Jul 2025 06:07:44 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8001756Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8001756Ki
  pods:               110
System Info:
  Machine ID:                 f4e79c9bbe1f458a9481b0f083e73ac8
  System UUID:                f4e79c9bbe1f458a9481b0f083e73ac8
  Boot ID:                    f15f5958-4358-4ed6-b685-79ce0f015ec5
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (24 in total)
  Namespace                   Name                                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                 ------------  ----------  ---------------  -------------  ---
  argocd                      argocd-application-controller-0                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  argocd                      argocd-applicationset-controller-655cc58ff8-2rb9p    0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  argocd                      argocd-dex-server-7d9dfb4fb8-kblv8                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  argocd                      argocd-notifications-controller-6c6848bc4c-cg7jf     0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  argocd                      argocd-redis-656c79549c-2cz8v                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  argocd                      argocd-repo-server-856b768fd9-5p2t7                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  argocd                      argocd-server-99c485944-hvqdz                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  cert-manager                cert-manager-69f748766f-spgbb                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h
  cert-manager                cert-manager-8555c66c68-4tpgq                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h
  cert-manager                cert-manager-cainjector-7cf6557c49-zhjw8             0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h
  cert-manager                cert-manager-webhook-58f4cff74d-kzcgz                0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h
  dev                         event-684974967c-54vht                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         53s
  dev                         event-684974967c-cqlnz                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         53s
  ingress-nginx               ingress-nginx-controller-67c5cb88f-5xw4j             100m (1%)     0 (0%)      90Mi (1%)        0 (0%)         18h
  kube-system                 coredns-674b8bbfcf-hhn6w                             100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     11d
  kube-system                 coredns-674b8bbfcf-p8cgl                             100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     11d
  kube-system                 etcd-minikube                                        100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         11d
  kube-system                 kube-apiserver-minikube                              250m (3%)     0 (0%)      0 (0%)           0 (0%)         11d
  kube-system                 kube-controller-manager-minikube                     200m (2%)     0 (0%)      0 (0%)           0 (0%)         11d
  kube-system                 kube-proxy-s7bwv                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         11d
  kube-system                 kube-scheduler-minikube                              100m (1%)     0 (0%)      0 (0%)           0 (0%)         11d
  kube-system                 storage-provisioner                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         11d
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-hx2jl           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-x994z                0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (11%)  0 (0%)
  memory             330Mi (4%)  340Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 16h                    kube-proxy       
  Normal   Starting                 2m52s                  kube-proxy       
  Normal   NodeAllocatableEnforced  16h                    kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  16h (x8 over 16h)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    16h (x8 over 16h)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     16h (x7 over 16h)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   Starting                 16h                    kubelet          Starting kubelet.
  Normal   RegisteredNode           16h                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 2m59s                  kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  2m59s (x8 over 2m59s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    2m59s (x8 over 2m59s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     2m59s (x7 over 2m59s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  2m59s                  kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 2m56s                  kubelet          Node minikube has been rebooted, boot id: f15f5958-4358-4ed6-b685-79ce0f015ec5
  Normal   RegisteredNode           2m53s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Aug 6 04:44] Hyper-V: Disabling IBT because of Hyper-V bug
[  +0.025676] PCI: Fatal: No config space access function found
[  +0.031384] PCI: System does not support PCI
[  +0.182946] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +1.957146] WSL (2 - init-systemd(Ubuntu)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.090806] pulseaudio[253]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.120463] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.003464] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000808] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000916] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000931] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.094248] Failed to connect to bus: No such file or directory
[  +0.309574] systemd-journald[52]: File /var/log/journal/6bb9a96e878d4546892489dc0372308e/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +3.711896] WSL (218) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +4.782612] WSL (2 - init-systemd(Ubuntu)) ERROR: WaitForBootProcess:3497: /sbin/init failed to start within 10000ms
[Aug 6 04:51] WSL (218) ERROR: CheckConnection: getaddrinfo() failed: -5
[Aug 6 05:00] WSL (218) ERROR: CheckConnection: getaddrinfo() failed: -5
[Aug 6 05:06] WSL (218) ERROR: CheckConnection: getaddrinfo() failed: -5
[Aug 6 05:09] WSL (218) ERROR: CheckConnection: getaddrinfo() failed: -5
[Aug 6 05:12] WSL (218) ERROR: CheckConnection: getaddrinfo() failed: -5


==> etcd [add380676697] <==
{"level":"warn","ts":"2025-08-05T12:57:37.972386Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"warn","ts":"2025-08-05T12:57:37.974087Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-08-05T12:57:37.974134Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.58.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.58.2:2380","--initial-cluster=minikube=https://192.168.58.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.58.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.58.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-08-05T12:57:37.974479Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-08-05T12:57:37.974551Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-08-05T12:57:37.974580Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2025-08-05T12:57:37.974707Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-08-05T12:57:37.978960Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"]}
{"level":"info","ts":"2025-08-05T12:57:37.979530Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-08-05T12:57:38.037351Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"56.939216ms"}
{"level":"info","ts":"2025-08-05T12:57:38.673543Z","caller":"etcdserver/server.go:513","msg":"recovered v2 store from snapshot","snapshot-index":80008,"snapshot-size":"11 kB"}
{"level":"info","ts":"2025-08-05T12:57:38.673700Z","caller":"etcdserver/server.go:526","msg":"recovered v3 backend from snapshot","backend-size-bytes":12423168,"backend-size":"12 MB","backend-size-in-use-bytes":5468160,"backend-size-in-use":"5.5 MB"}
{"level":"info","ts":"2025-08-05T12:57:38.827666Z","caller":"etcdserver/raft.go:541","msg":"restarting local member","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","commit-index":80368}
{"level":"info","ts":"2025-08-05T12:57:38.827853Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=(12882097698489969905)"}
{"level":"info","ts":"2025-08-05T12:57:38.828001Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became follower at term 13"}
{"level":"info","ts":"2025-08-05T12:57:38.828026Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b2c6679ac05f2cf1 [peers: [b2c6679ac05f2cf1], term: 13, commit: 80368, applied: 80008, lastindex: 80368, lastterm: 13]"}
{"level":"info","ts":"2025-08-05T12:57:38.837635Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-08-05T12:57:38.837796Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","recovered-remote-peer-id":"b2c6679ac05f2cf1","recovered-remote-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2025-08-05T12:57:38.837817Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-08-05T12:57:38.839603Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-08-05T12:57:38.841108Z","caller":"mvcc/kvstore.go:348","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":64784}
{"level":"info","ts":"2025-08-05T12:57:38.847907Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":65467}
{"level":"info","ts":"2025-08-05T12:57:38.848033Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":80368}
{"level":"info","ts":"2025-08-05T12:57:38.849941Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-08-05T12:57:38.853258Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"b2c6679ac05f2cf1","timeout":"7s"}
{"level":"info","ts":"2025-08-05T12:57:38.854718Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"b2c6679ac05f2cf1"}
{"level":"info","ts":"2025-08-05T12:57:38.854844Z","caller":"etcdserver/server.go:866","msg":"starting etcd server","local-member-id":"b2c6679ac05f2cf1","local-server-version":"3.5.21","cluster-id":"3a56e4ca95e2355c","cluster-version":"3.5"}
{"level":"info","ts":"2025-08-05T12:57:38.855088Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"b2c6679ac05f2cf1","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-08-05T12:57:38.855182Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-08-05T12:57:38.855386Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-08-05T12:57:38.855419Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-08-05T12:57:38.856960Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-08-05T12:57:38.862574Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-08-05T12:57:38.862807Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-08-05T12:57:38.862886Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-08-05T12:57:38.863130Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"b2c6679ac05f2cf1","initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-08-05T12:57:38.863161Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-08-05T12:57:39.838274Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 is starting a new election at term 13"}
{"level":"info","ts":"2025-08-05T12:57:39.838438Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became pre-candidate at term 13"}
{"level":"info","ts":"2025-08-05T12:57:39.838481Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgPreVoteResp from b2c6679ac05f2cf1 at term 13"}
{"level":"info","ts":"2025-08-05T12:57:39.838499Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became candidate at term 14"}
{"level":"info","ts":"2025-08-05T12:57:39.838546Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgVoteResp from b2c6679ac05f2cf1 at term 14"}
{"level":"info","ts":"2025-08-05T12:57:39.838557Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became leader at term 14"}
{"level":"info","ts":"2025-08-05T12:57:39.838564Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b2c6679ac05f2cf1 elected leader b2c6679ac05f2cf1 at term 14"}
{"level":"info","ts":"2025-08-05T12:57:39.854192Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"b2c6679ac05f2cf1","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.58.2:2379]}","request-path":"/0/members/b2c6679ac05f2cf1/attributes","cluster-id":"3a56e4ca95e2355c","publish-timeout":"7s"}
{"level":"info","ts":"2025-08-05T12:57:39.854306Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-08-05T12:57:39.854254Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-08-05T12:57:39.855478Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-08-05T12:57:39.855567Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-08-05T12:57:39.856892Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-08-05T12:57:39.856961Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-08-05T12:57:39.858754Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.58.2:2379"}
{"level":"info","ts":"2025-08-05T12:57:39.858864Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"warn","ts":"2025-08-05T12:58:03.408873Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"176.672218ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" limit:500 revision:65841 ","response":"range_response_count:9 size:1856056"}
{"level":"info","ts":"2025-08-05T12:58:03.408972Z","caller":"traceutil/trace.go:171","msg":"trace[1649574855] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:9; response_revision:65841; }","duration":"176.811779ms","start":"2025-08-05T12:58:03.232145Z","end":"2025-08-05T12:58:03.408957Z","steps":["trace[1649574855] 'range keys from bolt db'  (duration: 168.51309ms)"],"step_count":1}


==> etcd [d7fedce18f6a] <==
{"level":"warn","ts":"2025-08-06T05:13:50.376872Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"warn","ts":"2025-08-06T05:13:50.380986Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-08-06T05:13:50.381146Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.58.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.58.2:2380","--initial-cluster=minikube=https://192.168.58.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.58.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.58.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-08-06T05:13:50.381544Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-08-06T05:13:50.381653Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-08-06T05:13:50.381674Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2025-08-06T05:13:50.381738Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-08-06T05:13:50.384755Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"]}
{"level":"info","ts":"2025-08-06T05:13:50.385806Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-08-06T05:13:50.435744Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"48.862648ms"}
{"level":"info","ts":"2025-08-06T05:13:50.916316Z","caller":"etcdserver/server.go:513","msg":"recovered v2 store from snapshot","snapshot-index":80008,"snapshot-size":"11 kB"}
{"level":"info","ts":"2025-08-06T05:13:50.916408Z","caller":"etcdserver/server.go:526","msg":"recovered v3 backend from snapshot","backend-size-bytes":12423168,"backend-size":"12 MB","backend-size-in-use-bytes":8450048,"backend-size-in-use":"8.5 MB"}
{"level":"info","ts":"2025-08-06T05:13:51.081231Z","caller":"etcdserver/raft.go:541","msg":"restarting local member","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","commit-index":81416}
{"level":"info","ts":"2025-08-06T05:13:51.081492Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=(12882097698489969905)"}
{"level":"info","ts":"2025-08-06T05:13:51.081544Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became follower at term 14"}
{"level":"info","ts":"2025-08-06T05:13:51.081568Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b2c6679ac05f2cf1 [peers: [b2c6679ac05f2cf1], term: 14, commit: 81416, applied: 80008, lastindex: 81416, lastterm: 14]"}
{"level":"info","ts":"2025-08-06T05:13:51.081724Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-08-06T05:13:51.081761Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","recovered-remote-peer-id":"b2c6679ac05f2cf1","recovered-remote-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2025-08-06T05:13:51.081782Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-08-06T05:13:51.083522Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-08-06T05:13:51.085093Z","caller":"mvcc/kvstore.go:348","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":64784}
{"level":"info","ts":"2025-08-06T05:13:51.095517Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":66428}
{"level":"info","ts":"2025-08-06T05:13:51.095616Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":81416}
{"level":"info","ts":"2025-08-06T05:13:51.097144Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-08-06T05:13:51.099179Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"b2c6679ac05f2cf1","timeout":"7s"}
{"level":"info","ts":"2025-08-06T05:13:51.100356Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"b2c6679ac05f2cf1"}
{"level":"info","ts":"2025-08-06T05:13:51.100438Z","caller":"etcdserver/server.go:866","msg":"starting etcd server","local-member-id":"b2c6679ac05f2cf1","local-server-version":"3.5.21","cluster-id":"3a56e4ca95e2355c","cluster-version":"3.5"}
{"level":"info","ts":"2025-08-06T05:13:51.100578Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"b2c6679ac05f2cf1","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-08-06T05:13:51.101133Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-08-06T05:13:51.101191Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-08-06T05:13:51.101277Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-08-06T05:13:51.102037Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-08-06T05:13:51.104847Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-08-06T05:13:51.105016Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-08-06T05:13:51.105126Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-08-06T05:13:51.105250Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"b2c6679ac05f2cf1","initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-08-06T05:13:51.105436Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-08-06T05:13:51.282242Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 is starting a new election at term 14"}
{"level":"info","ts":"2025-08-06T05:13:51.282302Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became pre-candidate at term 14"}
{"level":"info","ts":"2025-08-06T05:13:51.282332Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgPreVoteResp from b2c6679ac05f2cf1 at term 14"}
{"level":"info","ts":"2025-08-06T05:13:51.282346Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became candidate at term 15"}
{"level":"info","ts":"2025-08-06T05:13:51.282384Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgVoteResp from b2c6679ac05f2cf1 at term 15"}
{"level":"info","ts":"2025-08-06T05:13:51.282399Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became leader at term 15"}
{"level":"info","ts":"2025-08-06T05:13:51.282425Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b2c6679ac05f2cf1 elected leader b2c6679ac05f2cf1 at term 15"}
{"level":"info","ts":"2025-08-06T05:13:51.296615Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"b2c6679ac05f2cf1","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.58.2:2379]}","request-path":"/0/members/b2c6679ac05f2cf1/attributes","cluster-id":"3a56e4ca95e2355c","publish-timeout":"7s"}
{"level":"info","ts":"2025-08-06T05:13:51.296646Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-08-06T05:13:51.296964Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-08-06T05:13:51.297011Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-08-06T05:13:51.296639Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-08-06T05:13:51.298880Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-08-06T05:13:51.298880Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-08-06T05:13:51.299731Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-08-06T05:13:51.299792Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.58.2:2379"}
{"level":"warn","ts":"2025-08-06T05:14:02.963546Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"163.592328ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" limit:500 revision:66689 ","response":"range_response_count:9 size:1856056"}
{"level":"info","ts":"2025-08-06T05:14:02.963658Z","caller":"traceutil/trace.go:171","msg":"trace[774152948] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:9; response_revision:66690; }","duration":"163.842082ms","start":"2025-08-06T05:14:02.799795Z","end":"2025-08-06T05:14:02.963638Z","steps":["trace[774152948] 'range keys from bolt db'  (duration: 162.680807ms)"],"step_count":1}


==> kernel <==
 05:16:49 up 32 min,  0 users,  load average: 0.80, 0.67, 0.29
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [8abdae0e8fb3] <==
I0806 05:13:52.412972       1 controller.go:90] Starting OpenAPI V3 controller
I0806 05:13:52.413003       1 naming_controller.go:299] Starting NamingConditionController
I0806 05:13:52.413033       1 establishing_controller.go:81] Starting EstablishingController
I0806 05:13:52.413476       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0806 05:13:52.413515       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0806 05:13:52.413639       1 crd_finalizer.go:269] Starting CRDFinalizer
I0806 05:13:52.411858       1 controller.go:78] Starting OpenAPI AggregationController
I0806 05:13:52.412859       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0806 05:13:52.413709       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0806 05:13:52.410008       1 aggregator.go:169] waiting for initial CRD sync...
I0806 05:13:52.412895       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0806 05:13:52.413789       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0806 05:13:52.412118       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0806 05:13:52.414994       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I0806 05:13:52.415070       1 shared_informer.go:350] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I0806 05:13:52.409983       1 local_available_controller.go:156] Starting LocalAvailability controller
I0806 05:13:52.415909       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0806 05:13:52.416902       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0806 05:13:52.416927       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0806 05:13:52.417225       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0806 05:13:52.416933       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0806 05:13:52.512223       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0806 05:13:52.512285       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0806 05:13:52.519572       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0806 05:13:52.515572       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0806 05:13:52.516386       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0806 05:13:52.518009       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0806 05:13:52.520031       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0806 05:13:52.514265       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0806 05:13:52.517956       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0806 05:13:52.517985       1 cache.go:39] Caches are synced for LocalAvailability controller
E0806 05:13:52.535192       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0806 05:13:52.539642       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0806 05:13:52.559885       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0806 05:13:52.575269       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0806 05:13:52.575754       1 policy_source.go:240] refreshing policies
I0806 05:13:52.584255       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0806 05:13:52.606832       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0806 05:13:52.610922       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0806 05:13:52.611118       1 aggregator.go:171] initial CRD sync complete...
I0806 05:13:52.611132       1 autoregister_controller.go:144] Starting autoregister controller
I0806 05:13:52.611141       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0806 05:13:52.611149       1 cache.go:39] Caches are synced for autoregister controller
I0806 05:13:52.614437       1 handler.go:288] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0806 05:13:52.614525       1 handler.go:288] Adding GroupVersion cert-manager.io v1 to ResourceManager
I0806 05:13:52.614549       1 handler.go:288] Adding GroupVersion acme.cert-manager.io v1 to ResourceManager
I0806 05:13:53.074639       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0806 05:13:53.426337       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0806 05:13:55.888658       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0806 05:13:55.996390       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0806 05:13:56.164742       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0806 05:13:56.189639       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0806 05:13:56.421688       1 controller.go:667] quota admission added evaluator for: endpoints
I0806 05:13:56.470581       1 controller.go:667] quota admission added evaluator for: statefulsets.apps
I0806 05:13:56.572381       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
E0806 05:14:02.743226       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 85a63c2f-d8c6-41a9-83f8-b3fb51ee9e0e, UID in object meta: "
I0806 05:14:23.728083       1 controller.go:667] quota admission added evaluator for: applications.argoproj.io
I0806 05:14:42.645848       1 controller.go:667] quota admission added evaluator for: namespaces
I0806 05:15:55.279284       1 alloc.go:328] "allocated clusterIPs" service="dev/event" clusterIPs={"IPv4":"10.106.1.90"}
I0806 05:15:55.300948       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-apiserver [bbc3a3968e04] <==
I0805 12:57:40.936531       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0805 12:57:40.937598       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0805 12:57:40.937337       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0805 12:57:40.937964       1 local_available_controller.go:156] Starting LocalAvailability controller
I0805 12:57:40.938073       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0805 12:57:40.938123       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0805 12:57:40.938130       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0805 12:57:40.938518       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0805 12:57:40.938534       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0805 12:57:40.938726       1 repairip.go:200] Starting ipallocator-repair-controller
I0805 12:57:40.938740       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0805 12:57:40.939597       1 controller.go:142] Starting OpenAPI controller
I0805 12:57:40.939670       1 controller.go:90] Starting OpenAPI V3 controller
I0805 12:57:40.939782       1 naming_controller.go:299] Starting NamingConditionController
I0805 12:57:40.939809       1 establishing_controller.go:81] Starting EstablishingController
I0805 12:57:40.939836       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0805 12:57:40.939849       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0805 12:57:40.939923       1 crd_finalizer.go:269] Starting CRDFinalizer
I0805 12:57:40.936513       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0805 12:57:40.937282       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I0805 12:57:40.940045       1 shared_informer.go:350] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I0805 12:57:40.940065       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0805 12:57:41.029103       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0805 12:57:41.037255       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0805 12:57:41.037519       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0805 12:57:41.037616       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0805 12:57:41.038101       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0805 12:57:41.038191       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0805 12:57:41.043265       1 cache.go:39] Caches are synced for LocalAvailability controller
I0805 12:57:41.043787       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0805 12:57:41.043969       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
E0805 12:57:41.043117       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0805 12:57:41.047030       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0805 12:57:41.050934       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0805 12:57:41.051708       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0805 12:57:41.067172       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0805 12:57:41.070272       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0805 12:57:41.070517       1 policy_source.go:240] refreshing policies
I0805 12:57:41.137562       1 handler.go:288] Adding GroupVersion cert-manager.io v1 to ResourceManager
I0805 12:57:41.137764       1 handler.go:288] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0805 12:57:41.137810       1 handler.go:288] Adding GroupVersion acme.cert-manager.io v1 to ResourceManager
I0805 12:57:41.139375       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0805 12:57:41.139532       1 aggregator.go:171] initial CRD sync complete...
I0805 12:57:41.139547       1 autoregister_controller.go:144] Starting autoregister controller
I0805 12:57:41.139557       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0805 12:57:41.139564       1 cache.go:39] Caches are synced for autoregister controller
I0805 12:57:41.154002       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0805 12:57:41.694555       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0805 12:57:41.971007       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0805 12:57:47.032311       1 controller.go:667] quota admission added evaluator for: endpoints
I0805 12:57:47.254663       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0805 12:57:47.272153       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0805 12:57:47.333961       1 controller.go:667] quota admission added evaluator for: statefulsets.apps
I0805 12:57:47.351670       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0805 12:57:47.395977       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0805 12:57:47.460062       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0805 12:57:47.518898       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0805 12:58:26.769050       1 controller.go:667] quota admission added evaluator for: applications.argoproj.io
I0805 13:01:58.304115       1 controller.go:667] quota admission added evaluator for: namespaces
I0805 13:02:03.455587       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [4f566a7ffba7] <==
I0806 05:13:55.634211       1 controllermanager.go:778] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0806 05:13:55.634244       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="volumeattributesclass-protection-controller" requiredFeatureGates=["VolumeAttributesClass"]
I0806 05:13:55.634320       1 pvc_protection_controller.go:168] "Starting PVC protection controller" logger="persistentvolumeclaim-protection-controller"
I0806 05:13:55.634331       1 shared_informer.go:350] "Waiting for caches to sync" controller="PVC protection"
I0806 05:13:55.683586       1 controllermanager.go:778] "Started controller" controller="legacy-serviceaccount-token-cleaner-controller"
I0806 05:13:55.687771       1 legacy_serviceaccount_token_cleaner.go:103] "Starting legacy service account token cleaner controller" logger="legacy-serviceaccount-token-cleaner-controller"
I0806 05:13:55.687841       1 shared_informer.go:350] "Waiting for caches to sync" controller="legacy-service-account-token-cleaner"
I0806 05:13:55.702613       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0806 05:13:55.745871       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0806 05:13:55.746941       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0806 05:13:55.751620       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0806 05:13:55.754394       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0806 05:13:55.760059       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0806 05:13:55.760086       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0806 05:13:55.760095       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0806 05:13:55.760107       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0806 05:13:55.771821       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0806 05:13:55.780048       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0806 05:13:55.780260       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0806 05:13:55.780323       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0806 05:13:55.800541       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0806 05:13:55.800702       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0806 05:13:55.805658       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0806 05:13:55.816407       1 shared_informer.go:357] "Caches are synced" controller="node"
I0806 05:13:55.816494       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0806 05:13:55.816684       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0806 05:13:55.816699       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0806 05:13:55.816707       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0806 05:13:55.821635       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0806 05:13:55.829638       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0806 05:13:55.835858       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0806 05:13:55.836107       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0806 05:13:55.836541       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0806 05:13:55.836679       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0806 05:13:55.836819       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0806 05:13:55.836908       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0806 05:13:55.837018       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0806 05:13:55.846023       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0806 05:13:55.860796       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0806 05:13:55.865638       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0806 05:13:55.868047       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0806 05:13:55.875480       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0806 05:13:55.883159       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0806 05:13:55.888813       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0806 05:13:55.889080       1 shared_informer.go:357] "Caches are synced" controller="job"
I0806 05:13:55.891495       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0806 05:13:55.909931       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0806 05:13:55.915955       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0806 05:13:55.924423       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0806 05:13:55.935046       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0806 05:13:55.993413       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0806 05:13:56.033963       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0806 05:13:56.071124       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0806 05:13:56.079535       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0806 05:13:56.302828       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0806 05:13:56.362180       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0806 05:13:56.528388       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0806 05:13:56.528480       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0806 05:13:56.528505       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0806 05:13:56.546825       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"


==> kube-controller-manager [e2b6b917777a] <==
I0805 12:57:45.206790       1 shared_informer.go:350] "Waiting for caches to sync" controller="taint-eviction-controller"
I0805 12:57:45.232832       1 controllermanager.go:778] "Started controller" controller="service-cidr-controller"
I0805 12:57:45.232892       1 controllermanager.go:756] "Warning: skipping controller" controller="storage-version-migrator-controller"
I0805 12:57:45.232901       1 controllermanager.go:736] "Skipping a cloud provider controller" controller="node-route-controller"
I0805 12:57:45.240210       1 servicecidrs_controller.go:136] "Starting" logger="service-cidr-controller" controller="service-cidr-controller"
I0805 12:57:45.240293       1 shared_informer.go:350] "Waiting for caches to sync" controller="service-cidr-controller"
I0805 12:57:45.332072       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0805 12:57:45.368993       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0805 12:57:45.373428       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0805 12:57:45.374515       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0805 12:57:46.636689       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0805 12:57:46.645430       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0805 12:57:46.645505       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0805 12:57:46.646101       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0805 12:57:46.646695       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0805 12:57:46.647149       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0805 12:57:46.648253       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0805 12:57:46.648964       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0805 12:57:46.648964       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0805 12:57:46.656170       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0805 12:57:46.656244       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0805 12:57:46.657691       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0805 12:57:46.662305       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0805 12:57:46.668494       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0805 12:57:46.669282       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0805 12:57:46.679906       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0805 12:57:46.693617       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0805 12:57:46.705277       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0805 12:57:46.706041       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0805 12:57:46.706080       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0805 12:57:46.706235       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0805 12:57:46.708963       1 shared_informer.go:357] "Caches are synced" controller="job"
I0805 12:57:46.712545       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0805 12:57:46.712597       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0805 12:57:46.719988       1 shared_informer.go:357] "Caches are synced" controller="node"
I0805 12:57:46.721520       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0805 12:57:46.721774       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0805 12:57:46.721876       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0805 12:57:46.721888       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0805 12:57:46.721897       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0805 12:57:46.726136       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0805 12:57:46.727841       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0805 12:57:46.727990       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0805 12:57:46.728093       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0805 12:57:46.730312       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0805 12:57:46.733607       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0805 12:57:46.734779       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0805 12:57:46.752312       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0805 12:57:46.752909       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0805 12:57:46.785159       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0805 12:57:46.785551       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0805 12:57:46.850393       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0805 12:57:46.869263       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0805 12:57:47.693780       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0805 12:57:47.780958       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0805 12:57:47.853337       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0805 12:57:47.853378       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0805 12:57:47.853394       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0805 12:57:47.935902       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0805 13:02:14.328054       1 namespace_controller.go:187] "Namespace has been deleted" logger="namespace-controller" namespace="dev"


==> kube-proxy [0bc444405bc6] <==
I0805 12:57:48.026720       1 server_linux.go:63] "Using iptables proxy"
I0805 12:57:48.450742       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
E0805 12:57:48.450996       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0805 12:57:48.562268       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0805 12:57:48.562404       1 server_linux.go:145] "Using iptables Proxier"
I0805 12:57:48.608719       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0805 12:57:48.615426       1 server.go:516] "Version info" version="v1.33.1"
I0805 12:57:48.615493       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0805 12:57:48.636520       1 config.go:199] "Starting service config controller"
I0805 12:57:48.636800       1 config.go:105] "Starting endpoint slice config controller"
I0805 12:57:48.638903       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0805 12:57:48.638984       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0805 12:57:48.641490       1 config.go:440] "Starting serviceCIDR config controller"
I0805 12:57:48.641514       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0805 12:57:48.641549       1 config.go:329] "Starting node config controller"
I0805 12:57:48.641557       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0805 12:57:48.741015       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0805 12:57:48.742485       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0805 12:57:48.742727       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0805 12:57:48.742998       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-proxy [d9baed162aa5] <==
I0806 05:13:56.059798       1 server_linux.go:63] "Using iptables proxy"
I0806 05:13:56.487561       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
E0806 05:13:56.509159       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0806 05:13:56.654517       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0806 05:13:56.654619       1 server_linux.go:145] "Using iptables Proxier"
I0806 05:13:56.673127       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0806 05:13:56.685395       1 server.go:516] "Version info" version="v1.33.1"
I0806 05:13:56.685470       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0806 05:13:56.700807       1 config.go:199] "Starting service config controller"
I0806 05:13:56.701503       1 config.go:329] "Starting node config controller"
I0806 05:13:56.702064       1 config.go:440] "Starting serviceCIDR config controller"
I0806 05:13:56.702477       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0806 05:13:56.702785       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0806 05:13:56.702835       1 config.go:105] "Starting endpoint slice config controller"
I0806 05:13:56.702843       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0806 05:13:56.704035       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0806 05:13:56.803551       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0806 05:13:56.803598       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0806 05:13:56.804459       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0806 05:13:56.805107       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [162da276ef8d] <==
I0806 05:13:51.075879       1 serving.go:386] Generated self-signed cert in-memory
W0806 05:13:52.480778       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0806 05:13:52.480850       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0806 05:13:52.480864       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0806 05:13:52.480872       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0806 05:13:52.533251       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0806 05:13:52.533300       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0806 05:13:52.537898       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0806 05:13:52.538151       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0806 05:13:52.538234       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0806 05:13:52.540106       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0806 05:13:52.640248       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [178222bfd0a9] <==
I0805 12:57:39.392801       1 serving.go:386] Generated self-signed cert in-memory
W0805 12:57:41.009871       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0805 12:57:41.009945       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0805 12:57:41.009962       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0805 12:57:41.009973       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0805 12:57:41.076402       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0805 12:57:41.076577       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0805 12:57:41.083197       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0805 12:57:41.084362       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0805 12:57:41.087673       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0805 12:57:41.087813       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0805 12:57:41.185515       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Aug 06 05:14:21 minikube kubelet[1689]: I0806 05:14:21.541267    1689 scope.go:117] "RemoveContainer" containerID="975941362e96481f9eb07b94a1f0b902107cc3b3b207b136be7f569e24f49a78"
Aug 06 05:14:22 minikube kubelet[1689]: I0806 05:14:22.542370    1689 scope.go:117] "RemoveContainer" containerID="25dfcd878e755332677315f5bc383b70d70259f12d555676656d65cee3da8c56"
Aug 06 05:14:25 minikube kubelet[1689]: I0806 05:14:25.092175    1689 scope.go:117] "RemoveContainer" containerID="bd5b8cb3ea1b8548858486874d2fc9e004952b19412a2c648fd9691155833b71"
Aug 06 05:14:25 minikube kubelet[1689]: E0806 05:14:25.092502    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 20s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:14:36 minikube kubelet[1689]: I0806 05:14:36.541618    1689 scope.go:117] "RemoveContainer" containerID="bd5b8cb3ea1b8548858486874d2fc9e004952b19412a2c648fd9691155833b71"
Aug 06 05:14:36 minikube kubelet[1689]: E0806 05:14:36.542069    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 20s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:14:49 minikube kubelet[1689]: I0806 05:14:49.077671    1689 scope.go:117] "RemoveContainer" containerID="bd5b8cb3ea1b8548858486874d2fc9e004952b19412a2c648fd9691155833b71"
Aug 06 05:14:50 minikube kubelet[1689]: I0806 05:14:50.132727    1689 scope.go:117] "RemoveContainer" containerID="bd5b8cb3ea1b8548858486874d2fc9e004952b19412a2c648fd9691155833b71"
Aug 06 05:14:50 minikube kubelet[1689]: I0806 05:14:50.133112    1689 scope.go:117] "RemoveContainer" containerID="ed9e5ab319fe269b74ea6e53b23be71616465842b2244708974ffd15b508cd72"
Aug 06 05:14:50 minikube kubelet[1689]: E0806 05:14:50.133300    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 40s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:14:56 minikube kubelet[1689]: I0806 05:14:56.627465    1689 scope.go:117] "RemoveContainer" containerID="ed9e5ab319fe269b74ea6e53b23be71616465842b2244708974ffd15b508cd72"
Aug 06 05:14:56 minikube kubelet[1689]: E0806 05:14:56.627788    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 40s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:15:09 minikube kubelet[1689]: I0806 05:15:09.077833    1689 scope.go:117] "RemoveContainer" containerID="ed9e5ab319fe269b74ea6e53b23be71616465842b2244708974ffd15b508cd72"
Aug 06 05:15:09 minikube kubelet[1689]: E0806 05:15:09.078179    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 40s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:15:21 minikube kubelet[1689]: I0806 05:15:21.678253    1689 scope.go:117] "RemoveContainer" containerID="ed9e5ab319fe269b74ea6e53b23be71616465842b2244708974ffd15b508cd72"
Aug 06 05:15:21 minikube kubelet[1689]: E0806 05:15:21.678635    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 40s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:15:36 minikube kubelet[1689]: I0806 05:15:36.679157    1689 scope.go:117] "RemoveContainer" containerID="ed9e5ab319fe269b74ea6e53b23be71616465842b2244708974ffd15b508cd72"
Aug 06 05:15:38 minikube kubelet[1689]: I0806 05:15:38.046781    1689 scope.go:117] "RemoveContainer" containerID="ed9e5ab319fe269b74ea6e53b23be71616465842b2244708974ffd15b508cd72"
Aug 06 05:15:38 minikube kubelet[1689]: I0806 05:15:38.047177    1689 scope.go:117] "RemoveContainer" containerID="ce8788d929680bed80b71fada7ee86eaa56135ba78be87417cdcae87f49928e9"
Aug 06 05:15:38 minikube kubelet[1689]: E0806 05:15:38.047374    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:15:39 minikube kubelet[1689]: I0806 05:15:39.092535    1689 scope.go:117] "RemoveContainer" containerID="ce8788d929680bed80b71fada7ee86eaa56135ba78be87417cdcae87f49928e9"
Aug 06 05:15:39 minikube kubelet[1689]: E0806 05:15:39.092848    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:15:52 minikube kubelet[1689]: I0806 05:15:52.678998    1689 scope.go:117] "RemoveContainer" containerID="ce8788d929680bed80b71fada7ee86eaa56135ba78be87417cdcae87f49928e9"
Aug 06 05:15:52 minikube kubelet[1689]: E0806 05:15:52.679444    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:15:55 minikube kubelet[1689]: I0806 05:15:55.397962    1689 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fkfmd\" (UniqueName: \"kubernetes.io/projected/c4ce760c-3b8d-4335-ae58-2088d51f2753-kube-api-access-fkfmd\") pod \"event-684974967c-cqlnz\" (UID: \"c4ce760c-3b8d-4335-ae58-2088d51f2753\") " pod="dev/event-684974967c-cqlnz"
Aug 06 05:15:55 minikube kubelet[1689]: I0806 05:15:55.499570    1689 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-m2bsf\" (UniqueName: \"kubernetes.io/projected/b7b310ab-a19d-4dc3-97d1-e6a297daafb3-kube-api-access-m2bsf\") pod \"event-684974967c-54vht\" (UID: \"b7b310ab-a19d-4dc3-97d1-e6a297daafb3\") " pod="dev/event-684974967c-54vht"
Aug 06 05:15:56 minikube kubelet[1689]: I0806 05:15:56.101260    1689 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="bd318ab8d3e996bf5d2c633cdd611b1831b161a902c230ccfe2cd73ac50797ff"
Aug 06 05:15:56 minikube kubelet[1689]: I0806 05:15:56.108125    1689 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d2ab5179ef875684df38fcc20af2085da5676159fc1f1a65345a713cf7476471"
Aug 06 05:15:59 minikube kubelet[1689]: E0806 05:15:59.288393    1689 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="event_new:v1"
Aug 06 05:15:59 minikube kubelet[1689]: E0806 05:15:59.289272    1689 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="event_new:v1"
Aug 06 05:15:59 minikube kubelet[1689]: E0806 05:15:59.291674    1689 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:event-app,Image:event_new:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2bsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod event-684974967c-54vht_dev(b7b310ab-a19d-4dc3-97d1-e6a297daafb3): ErrImagePull: Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Aug 06 05:15:59 minikube kubelet[1689]: E0806 05:15:59.292973    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"event-app\" with ErrImagePull: \"Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="dev/event-684974967c-54vht" podUID="b7b310ab-a19d-4dc3-97d1-e6a297daafb3"
Aug 06 05:16:00 minikube kubelet[1689]: E0806 05:16:00.270561    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"event-app\" with ImagePullBackOff: \"Back-off pulling image \\\"event_new:v1\\\": ErrImagePull: Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="dev/event-684974967c-54vht" podUID="b7b310ab-a19d-4dc3-97d1-e6a297daafb3"
Aug 06 05:16:02 minikube kubelet[1689]: E0806 05:16:02.037789    1689 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="event_new:v1"
Aug 06 05:16:02 minikube kubelet[1689]: E0806 05:16:02.037889    1689 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="event_new:v1"
Aug 06 05:16:02 minikube kubelet[1689]: E0806 05:16:02.038076    1689 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:event-app,Image:event_new:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkfmd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod event-684974967c-cqlnz_dev(c4ce760c-3b8d-4335-ae58-2088d51f2753): ErrImagePull: Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Aug 06 05:16:02 minikube kubelet[1689]: E0806 05:16:02.039407    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"event-app\" with ErrImagePull: \"Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="dev/event-684974967c-cqlnz" podUID="c4ce760c-3b8d-4335-ae58-2088d51f2753"
Aug 06 05:16:02 minikube kubelet[1689]: E0806 05:16:02.303826    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"event-app\" with ImagePullBackOff: \"Back-off pulling image \\\"event_new:v1\\\": ErrImagePull: Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="dev/event-684974967c-cqlnz" podUID="c4ce760c-3b8d-4335-ae58-2088d51f2753"
Aug 06 05:16:09 minikube kubelet[1689]: I0806 05:16:09.074346    1689 scope.go:117] "RemoveContainer" containerID="ce8788d929680bed80b71fada7ee86eaa56135ba78be87417cdcae87f49928e9"
Aug 06 05:16:09 minikube kubelet[1689]: E0806 05:16:09.074693    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:16:17 minikube kubelet[1689]: E0806 05:16:17.341405    1689 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="event_new:v1"
Aug 06 05:16:17 minikube kubelet[1689]: E0806 05:16:17.341572    1689 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="event_new:v1"
Aug 06 05:16:17 minikube kubelet[1689]: E0806 05:16:17.344206    1689 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:event-app,Image:event_new:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2bsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod event-684974967c-54vht_dev(b7b310ab-a19d-4dc3-97d1-e6a297daafb3): ErrImagePull: Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Aug 06 05:16:17 minikube kubelet[1689]: E0806 05:16:17.345715    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"event-app\" with ErrImagePull: \"Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="dev/event-684974967c-54vht" podUID="b7b310ab-a19d-4dc3-97d1-e6a297daafb3"
Aug 06 05:16:20 minikube kubelet[1689]: I0806 05:16:20.074525    1689 scope.go:117] "RemoveContainer" containerID="ce8788d929680bed80b71fada7ee86eaa56135ba78be87417cdcae87f49928e9"
Aug 06 05:16:20 minikube kubelet[1689]: E0806 05:16:20.074940    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:16:20 minikube kubelet[1689]: E0806 05:16:20.604354    1689 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="event_new:v1"
Aug 06 05:16:20 minikube kubelet[1689]: E0806 05:16:20.604468    1689 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="event_new:v1"
Aug 06 05:16:20 minikube kubelet[1689]: E0806 05:16:20.604747    1689 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:event-app,Image:event_new:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkfmd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod event-684974967c-cqlnz_dev(c4ce760c-3b8d-4335-ae58-2088d51f2753): ErrImagePull: Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Aug 06 05:16:20 minikube kubelet[1689]: E0806 05:16:20.606117    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"event-app\" with ErrImagePull: \"Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="dev/event-684974967c-cqlnz" podUID="c4ce760c-3b8d-4335-ae58-2088d51f2753"
Aug 06 05:16:31 minikube kubelet[1689]: E0806 05:16:31.475523    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"event-app\" with ImagePullBackOff: \"Back-off pulling image \\\"event_new:v1\\\": ErrImagePull: Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="dev/event-684974967c-54vht" podUID="b7b310ab-a19d-4dc3-97d1-e6a297daafb3"
Aug 06 05:16:32 minikube kubelet[1689]: I0806 05:16:32.471878    1689 scope.go:117] "RemoveContainer" containerID="ce8788d929680bed80b71fada7ee86eaa56135ba78be87417cdcae87f49928e9"
Aug 06 05:16:32 minikube kubelet[1689]: E0806 05:16:32.472370    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:16:36 minikube kubelet[1689]: E0806 05:16:36.475383    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"event-app\" with ImagePullBackOff: \"Back-off pulling image \\\"event_new:v1\\\": ErrImagePull: Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="dev/event-684974967c-cqlnz" podUID="c4ce760c-3b8d-4335-ae58-2088d51f2753"
Aug 06 05:16:45 minikube kubelet[1689]: I0806 05:16:45.471356    1689 scope.go:117] "RemoveContainer" containerID="ce8788d929680bed80b71fada7ee86eaa56135ba78be87417cdcae87f49928e9"
Aug 06 05:16:45 minikube kubelet[1689]: E0806 05:16:45.471780    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cert-manager-controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cert-manager-controller pod=cert-manager-8555c66c68-4tpgq_cert-manager(0d769dd2-f560-4f28-97bd-89438f5f9f73)\"" pod="cert-manager/cert-manager-8555c66c68-4tpgq" podUID="0d769dd2-f560-4f28-97bd-89438f5f9f73"
Aug 06 05:16:45 minikube kubelet[1689]: E0806 05:16:45.681129    1689 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="event_new:v1"
Aug 06 05:16:45 minikube kubelet[1689]: E0806 05:16:45.681237    1689 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="event_new:v1"
Aug 06 05:16:45 minikube kubelet[1689]: E0806 05:16:45.681410    1689 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:event-app,Image:event_new:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2bsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod event-684974967c-54vht_dev(b7b310ab-a19d-4dc3-97d1-e6a297daafb3): ErrImagePull: Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Aug 06 05:16:45 minikube kubelet[1689]: E0806 05:16:45.682803    1689 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"event-app\" with ErrImagePull: \"Error response from daemon: pull access denied for event_new, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="dev/event-684974967c-54vht" podUID="b7b310ab-a19d-4dc3-97d1-e6a297daafb3"


==> kubernetes-dashboard [4f3cdb66deb2] <==
2025/08/06 05:14:16 Using namespace: kubernetes-dashboard
2025/08/06 05:14:16 Using in-cluster config to connect to apiserver
2025/08/06 05:14:16 Using secret token for csrf signing
2025/08/06 05:14:16 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/08/06 05:14:16 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2025/08/06 05:14:16 Successful initial request to the apiserver, version: v1.33.1
2025/08/06 05:14:16 Generating JWE encryption key
2025/08/06 05:14:16 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2025/08/06 05:14:16 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2025/08/06 05:14:16 Initializing JWE encryption key from synchronized object
2025/08/06 05:14:16 Creating in-cluster Sidecar client
2025/08/06 05:14:17 Serving insecurely on HTTP port: 9090
2025/08/06 05:14:17 Successful request to sidecar
2025/08/06 05:14:16 Starting overwatch


==> kubernetes-dashboard [adacaee3a362] <==
2025/08/06 05:13:55 Using namespace: kubernetes-dashboard
2025/08/06 05:13:55 Using in-cluster config to connect to apiserver
2025/08/06 05:13:55 Using secret token for csrf signing
2025/08/06 05:13:55 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/08/06 05:13:55 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": net/http: TLS handshake timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc0006dfae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000196a80)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf


==> storage-provisioner [6fde69e8e7b3] <==
W0806 05:15:47.085900       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:47.091984       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:49.096822       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:49.117912       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:51.122232       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:51.138837       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:53.143752       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:53.161012       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:56.562096       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:56.568572       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:58.575737       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:15:58.590895       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:00.596965       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:00.616966       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:02.620875       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:02.637238       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:04.641408       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:04.648171       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:06.652996       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:06.663906       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:08.668833       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:08.686772       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:10.691254       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:10.709581       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:12.715008       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:12.727000       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:14.732992       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:14.740389       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:16.746142       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:16.757709       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:18.764530       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:18.784008       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:20.788667       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:20.804934       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:22.809575       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:22.830446       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:24.837199       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:24.858228       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:26.862353       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:26.880343       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:30.282911       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:30.288238       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:32.294299       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:32.311656       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:34.316360       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:34.329889       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:36.333024       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:36.347611       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:38.351508       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:38.366474       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:40.372667       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:40.389781       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:42.395819       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:42.410507       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:44.413539       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:44.426965       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:46.433039       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:46.449845       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:48.455391       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0806 05:16:48.468737       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [975941362e96] <==
I0806 05:13:55.561503       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0806 05:14:05.768832       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

